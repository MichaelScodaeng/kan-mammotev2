{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e349f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Setting up LSTM Time Embedding Comparison...\n",
      "ğŸ”§ Using device: cuda\n",
      "âœ… Setup complete! All imports and model definitions loaded.\n",
      "âœ… Device: cuda\n",
      "âœ… Results directory: /home/s2516027/kan-mammote/results\n",
      "ğŸš€ Ready to run the comparison!\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ LSTM Time Embedding Comparison - Complete Setup Cell\n",
    "print(\"ğŸ”§ Setting up LSTM Time Embedding Comparison...\")\n",
    "\n",
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep learning imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.auto import tqdm # Import tqdm\n",
    "\n",
    "# Add project root to path\n",
    "project_root = '/home/s2516027/kan-mammote'\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Import project modules\n",
    "from src.models.kan_mammote import KANMAMMOTE\n",
    "from src.utils.config import KANMAMOTEConfig\n",
    "from src.LETE.LeTE import CombinedLeTE as LeTE # Import CombinedLeTE and alias it as LeTE\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10\n",
    "LSTM_HIDDEN_DIM = 128\n",
    "TIME_EMBEDDING_DIM = 64\n",
    "DROPOUT_RATE = 0.2\n",
    "THRESHOLD = 0.3\n",
    "GRAD_CLIP_NORM = 1.0\n",
    "\n",
    "# Setup device and directories\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ğŸ”§ Using device: {device}\")\n",
    "\n",
    "RESULTS_DIR = f\"{project_root}/results\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "class EventBasedMNIST(Dataset):\n",
    "    \"\"\"Convert MNIST to event-based representation.\"\"\"\n",
    "    \n",
    "    def __init__(self, root, train=True, threshold=0.3, download=False):\n",
    "        self.threshold = threshold\n",
    "        transform = transforms.Compose([transforms.ToTensor()])\n",
    "        self.mnist = datasets.MNIST(root=root, train=train, download=download, transform=transform)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.mnist)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.mnist[idx]\n",
    "        \n",
    "        # Convert to event representation\n",
    "        image = image.squeeze().numpy()\n",
    "        \n",
    "        # Create events for pixels above threshold\n",
    "        events = []\n",
    "        features = []\n",
    "        \n",
    "        for i in range(28):\n",
    "            for j in range(28):\n",
    "                if image[i, j] > self.threshold:\n",
    "                    position = i * 28 + j  # Flatten position\n",
    "                    events.append(position)\n",
    "                    features.append(image[i, j])\n",
    "        \n",
    "        if len(events) == 0:\n",
    "            events = [0]\n",
    "            features = [0.0]\n",
    "        \n",
    "        return np.array(events), np.array(features), label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for variable length sequences.\"\"\"\n",
    "    events_batch = []\n",
    "    features_batch = []\n",
    "    lengths = []\n",
    "    labels = []\n",
    "    \n",
    "    max_len = max(len(events) for events, _, _ in batch)\n",
    "    \n",
    "    for events, features, label in batch:\n",
    "        length = len(events)\n",
    "        lengths.append(length)\n",
    "        labels.append(label)\n",
    "        \n",
    "        # Pad sequences\n",
    "        padded_events = np.zeros(max_len, dtype=np.int64)\n",
    "        padded_features = np.zeros(max_len, dtype=np.float32)\n",
    "        \n",
    "        padded_events[:length] = events\n",
    "        padded_features[:length] = features\n",
    "        \n",
    "        events_batch.append(padded_events)\n",
    "        features_batch.append(padded_features)\n",
    "    \n",
    "    return (torch.tensor(events_batch), \n",
    "            torch.tensor(features_batch), \n",
    "            torch.tensor(lengths), \n",
    "            torch.tensor(labels))\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable parameters in a model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class TrueBaselineLSTM(nn.Module):\n",
    "    \"\"\"True baseline LSTM without any time embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=784, hidden_dim=128, num_classes=10, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Just use feature values directly\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1,  # Just the pixel values\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, events, features, lengths):\n",
    "        if features.size(0) == 0:\n",
    "            return torch.zeros(0, 10, device=features.device)\n",
    "        \n",
    "        lengths = torch.clamp(lengths, min=1, max=features.size(1))\n",
    "        \n",
    "        # Use only pixel values, no positional information\n",
    "        x = features.unsqueeze(-1)  # Add feature dimension\n",
    "        \n",
    "        # Pack sequences\n",
    "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (h_n, c_n) = self.lstm(packed)\n",
    "        \n",
    "        # Classify using last hidden state\n",
    "        output = self.classifier(h_n[-1])\n",
    "        return output\n",
    "\n",
    "class LearnablePositionLSTM(nn.Module):\n",
    "    \"\"\"LSTM with learnable position embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=784, hidden_dim=128, num_classes=10, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # Learnable position embedding\n",
    "        self.position_embedding = nn.Embedding(input_size, TIME_EMBEDDING_DIM)\n",
    "        \n",
    "        # Feature projection\n",
    "        self.feature_proj = nn.Linear(1, TIME_EMBEDDING_DIM)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=TIME_EMBEDDING_DIM,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, events, features, lengths):\n",
    "        if events.size(0) == 0:\n",
    "            return torch.zeros(0, 10, device=events.device)\n",
    "        \n",
    "        lengths = torch.clamp(lengths, min=1, max=events.size(1))\n",
    "        \n",
    "        # Get position embeddings\n",
    "        pos_emb = self.position_embedding(events)\n",
    "        \n",
    "        # Project features\n",
    "        feat_emb = self.feature_proj(features.unsqueeze(-1))\n",
    "        \n",
    "        # Combine position and feature embeddings\n",
    "        combined = pos_emb + feat_emb\n",
    "        \n",
    "        # Pack sequences\n",
    "        packed = pack_padded_sequence(combined, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (h_n, c_n) = self.lstm(packed)\n",
    "        \n",
    "        # Classify\n",
    "        output = self.classifier(h_n[-1])\n",
    "        return output\n",
    "\n",
    "class SinCosLSTM(nn.Module):\n",
    "    \"\"\"LSTM with sinusoidal position embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=784, hidden_dim=128, num_classes=10, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Feature projection\n",
    "        self.feature_proj = nn.Linear(1, TIME_EMBEDDING_DIM)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=TIME_EMBEDDING_DIM,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def get_sincos_embeddings(self, positions):\n",
    "        \"\"\"Generate sinusoidal position embeddings.\"\"\"\n",
    "        batch_size, seq_len = positions.shape\n",
    "        embeddings = torch.zeros(batch_size, seq_len, TIME_EMBEDDING_DIM, device=positions.device)\n",
    "        \n",
    "        # Normalize positions to [0, 1]\n",
    "        max_pos = positions.max(dim=1, keepdim=True)[0].float()\n",
    "        normalized_pos = positions.float() / (max_pos + 1e-8)\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, TIME_EMBEDDING_DIM, 2, device=positions.device).float() * \n",
    "                            -(np.log(10000.0) / TIME_EMBEDDING_DIM))\n",
    "        \n",
    "        pos_scaled = normalized_pos.unsqueeze(-1) * div_term.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        embeddings[:, :, 0::2] = torch.sin(pos_scaled)\n",
    "        embeddings[:, :, 1::2] = torch.cos(pos_scaled)\n",
    "        \n",
    "        return embeddings\n",
    "        \n",
    "    def forward(self, events, features, lengths):\n",
    "        if events.size(0) == 0:\n",
    "            return torch.zeros(0, 10, device=events.device)\n",
    "        \n",
    "        lengths = torch.clamp(lengths, min=1, max=events.size(1))\n",
    "        \n",
    "        # Get sinusoidal position embeddings\n",
    "        pos_emb = self.get_sincos_embeddings(events)\n",
    "        \n",
    "        # Project features\n",
    "        feat_emb = self.feature_proj(features.unsqueeze(-1))\n",
    "        \n",
    "        # Combine embeddings\n",
    "        combined = pos_emb + feat_emb\n",
    "        \n",
    "        # Pack sequences\n",
    "        packed = pack_padded_sequence(combined, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (h_n, c_n) = self.lstm(packed)\n",
    "        \n",
    "        # Classify\n",
    "        output = self.classifier(h_n[-1])\n",
    "        return output\n",
    "\n",
    "class LETE_LSTM_Fixed(nn.Module):\n",
    "    \"\"\"LSTM with LETE time embeddings - FIXED.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=784, hidden_dim=128, num_classes=10, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Initialize LETE\n",
    "        self.lete = LeTE(dim=TIME_EMBEDDING_DIM)\n",
    "        \n",
    "        # Feature projection\n",
    "        self.feature_proj = nn.Linear(1, TIME_EMBEDDING_DIM)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=TIME_EMBEDDING_DIM,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, events, features, lengths):\n",
    "        # Input validation\n",
    "        if events.size(0) == 0:\n",
    "            return torch.zeros(0, 10, device=events.device)\n",
    "        \n",
    "        # Clamp lengths to prevent issues\n",
    "        lengths = torch.clamp(lengths, min=1, max=events.size(1))\n",
    "        \n",
    "        # Normalize timestamps\n",
    "        max_pos = events.max(dim=1, keepdim=True)[0].float()\n",
    "        # Prevent division by zero\n",
    "        max_pos[max_pos == 0] = 1.0\n",
    "        timestamps = events.float() / max_pos\n",
    "        \n",
    "        # Get LETE embeddings\n",
    "        timestamps_3d = timestamps.unsqueeze(-1)\n",
    "        lete_emb = self.lete(timestamps_3d)\n",
    "        \n",
    "        # Handle shape mismatches\n",
    "        batch_size, seq_len = timestamps.shape\n",
    "        if lete_emb.shape != (batch_size, seq_len, TIME_EMBEDDING_DIM):\n",
    "            if lete_emb.dim() == 3:\n",
    "                # Take only what we need\n",
    "                lete_emb = lete_emb[:, :seq_len, :TIME_EMBEDDING_DIM]\n",
    "            else:\n",
    "                # Reshape as needed\n",
    "                lete_emb = lete_emb.view(batch_size, seq_len, -1)\n",
    "                if lete_emb.shape[2] != TIME_EMBEDDING_DIM:\n",
    "                    # Project to correct dimension\n",
    "                    if not hasattr(self, 'lete_proj'):\n",
    "                        self.lete_proj = nn.Linear(lete_emb.shape[2], TIME_EMBEDDING_DIM).to(lete_emb.device)\n",
    "                    lete_emb = self.lete_proj(lete_emb)\n",
    "        \n",
    "        # Clip to prevent numerical issues\n",
    "        lete_emb = torch.clamp(lete_emb, min=-1e3, max=1e3)\n",
    "        \n",
    "        # Project features\n",
    "        feat_emb = self.feature_proj(features.unsqueeze(-1))\n",
    "        \n",
    "        # Combine embeddings\n",
    "        combined = lete_emb + feat_emb\n",
    "        combined = F.relu(combined)\n",
    "        \n",
    "        # LSTM processing\n",
    "        packed = pack_padded_sequence(combined, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (h_n, c_n) = self.lstm(packed)\n",
    "        \n",
    "        # Classify\n",
    "        output = self.classifier(h_n[-1])\n",
    "        return output\n",
    "class KAN_MAMMOTE_LSTM_Fixed(nn.Module):\n",
    "    \"\"\"LSTM with KAN-MAMMOTE - NO FALLBACKS.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=784, hidden_dim=128, num_classes=10, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.timestamp_proj = nn.Linear(1, 2)  # Project from dim 1 to 2 (minimal dimensionality for router)\n",
    "        # Initialize KAN-MAMMOTE - NO FALLBACK\n",
    "        self.kan_config = KANMAMOTEConfig(\n",
    "            D_time=TIME_EMBEDDING_DIM,\n",
    "            d_model=LSTM_HIDDEN_DIM,\n",
    "            input_feature_dim=1,  # Single feature dimension \n",
    "            output_dim_for_task=TIME_EMBEDDING_DIM,  # Match the dimension needed for LSTM\n",
    "            K_top=4,\n",
    "            use_aux_features_router=False,  # Set to False since we're not using auxiliary features\n",
    "            raw_event_feature_dim=1,\n",
    "            num_layers=1  # Use only one layer for simplicity\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            print(\"ğŸ”„ Initializing KAN-MAMMOTE...\")\n",
    "            self.kan_mammote = KANMAMMOTE(self.kan_config)\n",
    "            print(\"âœ… KAN-MAMMOTE initialized successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error initializing KAN-MAMMOTE: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "        \n",
    "        # Feature projection\n",
    "        self.feature_proj = nn.Linear(1, TIME_EMBEDDING_DIM)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        self.combine = nn.Linear(TIME_EMBEDDING_DIM * 2, TIME_EMBEDDING_DIM)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=TIME_EMBEDDING_DIM,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, events, features, lengths):\n",
    "        # Debug info\n",
    "        #print(f\"ğŸ“Š Debug - Input shapes: events={events.shape}, features={features.shape}, lengths={lengths.shape}\")\n",
    "        \n",
    "        # Input validation\n",
    "        if events.size(0) == 0:\n",
    "            print(\"âš ï¸ Empty batch detected!\")\n",
    "            return torch.zeros(0, 10, device=events.device)\n",
    "        \n",
    "        if lengths.max() == 0:\n",
    "            print(\"âš ï¸ All zero-length sequences detected!\")\n",
    "            return torch.zeros(events.size(0), 10, device=events.device)\n",
    "        \n",
    "        # Clamp lengths to prevent issues\n",
    "        lengths = torch.clamp(lengths, min=1, max=events.size(1))\n",
    "        #print(f\"ğŸ“Š Debug - Clamped lengths: min={lengths.min().item()}, max={lengths.max().item()}\")\n",
    "        \n",
    "        # Filter valid sequences\n",
    "        valid_mask = lengths > 0\n",
    "        if not valid_mask.any():\n",
    "            #print(\"âš ï¸ No valid sequences after filtering!\")\n",
    "            return torch.zeros(events.size(0), 10, device=events.device)\n",
    "        \n",
    "        events_valid = events[valid_mask]\n",
    "        features_valid = features[valid_mask]\n",
    "        lengths_valid = lengths[valid_mask]\n",
    "        \n",
    "        #print(f\"ğŸ“Š Debug - Valid data shapes: events={events_valid.shape}, features={features_valid.shape}, lengths={lengths_valid.shape}\")\n",
    "        \n",
    "        # Normalize positions\n",
    "        max_pos = events_valid.max(dim=1, keepdim=True)[0].float()\n",
    "        #print(f\"ğŸ“Š Debug - Max positions: shape={max_pos.shape}, min={max_pos.min().item()}, max={max_pos.max().item()}\")\n",
    "        \n",
    "        # Check for zeros in max_pos\n",
    "        if (max_pos == 0).any():\n",
    "            #print(\"âš ï¸ Zero max positions detected! Applying fix...\")\n",
    "            max_pos[max_pos == 0] = 1.0\n",
    "            \n",
    "        timestamps = events_valid.float() / (max_pos + 1e-8)\n",
    "        #print(f\"ğŸ“Š Debug - Timestamps: shape={timestamps.shape}, min={timestamps.min().item()}, max={timestamps.max().item()}\")\n",
    "        \n",
    "        # Reshape data for KAN-MAMMOTE - fix matrix multiplication issue\n",
    "        # We need to reshape data to match what KANMAMMOTE expects:\n",
    "        # timestamps should be shape (batch_size, seq_len)\n",
    "        # features should be shape (batch_size, seq_len, feature_dim)\n",
    "        \n",
    "        # Extract only the first element from each sequence to make a simpler test case\n",
    "        # This avoids the shape mismatch by reducing the problem to a single timestep per sample\n",
    "        batch_size = events_valid.shape[0]\n",
    "        \n",
    "        # Use just the first valid timestamp from each sequence\n",
    "        timestamps_single = timestamps[:, 0].unsqueeze(1)  # Shape: (batch_size, 1)\n",
    "        #print(f\"ğŸ“Š Debug - Single timestamps: shape={timestamps_single.shape}\")\n",
    "        \n",
    "        # Project timestamps to match expected dimension\n",
    "        timestamps_projected = self.timestamp_proj(timestamps_single)  # Shape: (batch_size, 64)\n",
    "        #print(f\"ğŸ“Š Debug - Projected timestamps: shape={timestamps_projected.shape}\")\n",
    "        \n",
    "        # Use just the first feature from each sequence\n",
    "        features_single = features_valid[:, 0].unsqueeze(1).unsqueeze(1)  # Shape: (batch_size, 1, 1)\n",
    "        #print(f\"ğŸ“Š Debug - Single features: shape={features_single.shape}\")\n",
    "        \n",
    "        # Check for NaNs\n",
    "        if torch.isnan(timestamps_projected).any():\n",
    "            #print(\"âš ï¸ NaNs detected in projected timestamps!\")\n",
    "            raise ValueError(\"NaN values detected in projected timestamps\")\n",
    "            \n",
    "        if torch.isnan(features_single).any():\n",
    "            #print(\"âš ï¸ NaNs detected in features!\")\n",
    "            raise ValueError(\"NaN values detected in features\")\n",
    "        \n",
    "        # KAN-MAMMOTE forward pass\n",
    "        #print(\"ğŸ”„ Running KAN-MAMMOTE forward pass...\")\n",
    "        kan_output, debug_info = self.kan_mammote(timestamps_projected, features_single)\n",
    "        #print(f\"âœ… KAN-MAMMOTE output shape: {kan_output.shape}\")\n",
    "        \n",
    "        # If debug info available, print it\n",
    "        if debug_info:\n",
    "            #print(f\"ğŸ“Š KAN-MAMMOTE debug info: {debug_info}\")\n",
    "            pass\n",
    "        \n",
    "        # Create embeddings for all sequence positions\n",
    "        # For simplicity, we'll repeat the KAN embedding for each position in the sequence\n",
    "        if kan_output.dim() > 2:\n",
    "            # Flatten any intermediate dimensions while keeping batch and feature dims\n",
    "            kan_output = kan_output.view(kan_output.size(0), -1)\n",
    "            #print(f\"ğŸ“Š Debug - Reshaped KAN output: shape={kan_output.shape}\")\n",
    "            \n",
    "        # Now expand along sequence dimension\n",
    "        kan_emb = kan_output.unsqueeze(1).expand(-1, events_valid.shape[1], -1)\n",
    "        #print(f\"ğŸ“Š Debug - Expanded KAN embeddings: shape={kan_emb.shape}\")\n",
    "        \n",
    "        # Project features for the second pathway\n",
    "        feat_emb = self.feature_proj(features_valid.unsqueeze(-1))\n",
    "        #print(f\"ğŸ“Š Debug - Feature embeddings: shape={feat_emb.shape}\")\n",
    "        \n",
    "        # Combine embeddings from both pathways\n",
    "        combined = torch.cat([kan_emb, feat_emb], dim=-1)\n",
    "        combined = self.combine(combined)\n",
    "        combined = F.relu(combined)\n",
    "        #print(f\"ğŸ“Š Debug - Combined embeddings: shape={combined.shape}\")\n",
    "        \n",
    "        # Check for NaNs in combined embedding\n",
    "        if torch.isnan(combined).any():\n",
    "            #print(\"âš ï¸ NaNs detected in combined embeddings!\")\n",
    "            raise ValueError(\"NaN values detected in combined embeddings\")\n",
    "        \n",
    "        # LSTM processing\n",
    "        #print(f\"ğŸ”„ Packing sequences with lengths: {lengths_valid}\")\n",
    "        packed = pack_padded_sequence(combined, lengths_valid.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (h_n, c_n) = self.lstm(packed)\n",
    "        #print(f\"ğŸ“Š Debug - LSTM hidden state: shape={h_n.shape}\")\n",
    "        \n",
    "        # Classifier\n",
    "        valid_logits = self.classifier(h_n[-1])\n",
    "        #print(f\"ğŸ“Š Debug - Valid logits: shape={valid_logits.shape}\")\n",
    "        \n",
    "        # Create full output\n",
    "        full_logits = torch.zeros(events.size(0), 10, device=events.device)\n",
    "        full_logits[valid_mask] = valid_logits\n",
    "        #print(f\"ğŸ“Š Debug - Full output: shape={full_logits.shape}\")\n",
    "        \n",
    "        return full_logits\n",
    "\n",
    "def train_model(model, train_loader, test_loader, model_name):\n",
    "    \"\"\"Train a single model with immediate error stopping.\"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': [],\n",
    "        'epochs': [], 'training_time': []\n",
    "    }\n",
    "    \n",
    "    best_test_acc = 0.0\n",
    "    \n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\", leave=False)\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (events, features, lengths, labels) in enumerate(train_pbar):\n",
    "            # No try-except here - let errors propagate and stop execution\n",
    "            events, features, lengths, labels = events.to(device), features.to(device), lengths.to(device), labels.to(device)\n",
    "            \n",
    "            # Debug info for first few batches\n",
    "            '''if batch_idx < 3:\n",
    "                print(f\"\\nğŸ“Š Batch {batch_idx} debug:\")\n",
    "                print(f\"  - events: shape={events.shape}, range=[{events.min().item()}, {events.max().item()}]\")\n",
    "                print(f\"  - features: shape={features.shape}, range=[{features.min().item()}, {features.max().item()}]\")\n",
    "                print(f\"  - lengths: shape={lengths.shape}, range=[{lengths.min().item()}, {lengths.max().item()}]\")\n",
    "                print(f\"  - labels: shape={labels.shape}, values={labels.tolist()}\")'''\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(events, features, lengths)\n",
    "            \n",
    "            # Check outputs and raise error if issues are found\n",
    "            if torch.isnan(outputs).any():\n",
    "                print(f\"âš ï¸ NaN detected in outputs at batch {batch_idx}!\")\n",
    "                raise ValueError(f\"NaN detected in model outputs at batch {batch_idx}\")\n",
    "                \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Check loss and raise error if issues are found\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"âš ï¸ NaN/Inf loss detected at batch {batch_idx}: {loss.item()}\")\n",
    "                raise ValueError(f\"NaN or Inf loss value detected at batch {batch_idx}: {loss.item()}\")\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # Check gradients and raise error if issues are found\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None and (torch.isnan(param.grad).any() or torch.isinf(param.grad).any()):\n",
    "                    print(f\"âš ï¸ NaN/Inf gradient detected in {name}\")\n",
    "                    raise ValueError(f\"NaN or Inf gradient detected in parameter {name}\")\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_NORM)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update tqdm postfix\n",
    "            train_pbar.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{100*train_correct/train_total:.2f}%\")\n",
    "        \n",
    "        train_pbar.close()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        test_pbar = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Test]\", leave=False)\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (events, features, lengths, labels) in enumerate(test_pbar):\n",
    "                # No try-except here - let errors propagate and stop execution\n",
    "                events, features, lengths, labels = events.to(device), features.to(device), lengths.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(events, features, lengths)\n",
    "                \n",
    "                # Check outputs and raise error if issues are found\n",
    "                if torch.isnan(outputs).any():\n",
    "                    print(f\"âš ï¸ NaN detected in test outputs at batch {batch_idx}!\")\n",
    "                    raise ValueError(f\"NaN detected in test model outputs at batch {batch_idx}\")\n",
    "                    \n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Check loss and raise error if issues are found\n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    print(f\"âš ï¸ NaN/Inf test loss detected at batch {batch_idx}: {loss.item()}\")\n",
    "                    raise ValueError(f\"NaN or Inf test loss value detected at batch {batch_idx}: {loss.item()}\")\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                test_pbar.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{100*test_correct/test_total:.2f}%\")\n",
    "        \n",
    "        test_pbar.close()\n",
    "\n",
    "        # Calculate metrics\n",
    "        epoch_time = time.time() - start_time\n",
    "        train_acc = 100 * train_correct / max(train_total, 1)  # Avoid division by zero\n",
    "        test_acc = 100 * test_correct / max(test_total, 1)    # Avoid division by zero\n",
    "        avg_train_loss = train_loss / max(len(train_loader), 1)  # Avoid division by zero\n",
    "        avg_test_loss = test_loss / max(len(test_loader), 1)    # Avoid division by zero\n",
    "        \n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "        \n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_loss'].append(avg_test_loss)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        history['epochs'].append(epoch + 1)\n",
    "        history['training_time'].append(epoch_time)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{NUM_EPOCHS}: Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%, Time: {epoch_time:.1f}s')\n",
    "    \n",
    "    print(f\"âœ… {model_name} training complete. Best test accuracy: {best_test_acc:.2f}%\")\n",
    "    return history, best_test_acc\n",
    "\n",
    "def plot_training_curves(results):\n",
    "    \"\"\"Plot training curves for all models.\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot training accuracy\n",
    "    plt.subplot(2, 2, 1)\n",
    "    for model_name, history in results.items():\n",
    "        if 'train_acc' in history and len(history['train_acc']) > 0:\n",
    "            plt.plot(history['epochs'], history['train_acc'], label=model_name, marker='o')\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot test accuracy\n",
    "    plt.subplot(2, 2, 2)\n",
    "    for model_name, history in results.items():\n",
    "        if 'test_acc' in history and len(history['test_acc']) > 0:\n",
    "            plt.plot(history['epochs'], history['test_acc'], label=model_name, marker='o')\n",
    "    plt.title('Test Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.subplot(2, 2, 3)\n",
    "    for model_name, history in results.items():\n",
    "        if 'train_loss' in history and len(history['train_loss']) > 0:\n",
    "            plt.plot(history['epochs'], history['train_loss'], label=model_name, marker='o')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot test loss\n",
    "    plt.subplot(2, 2, 4)\n",
    "    for model_name, history in results.items():\n",
    "        if 'test_loss' in history and len(history['test_loss']) > 0:\n",
    "            plt.plot(history['epochs'], history['test_loss'], label=model_name, marker='o')\n",
    "    plt.title('Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{RESULTS_DIR}/training_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"âœ… Training curves saved to {RESULTS_DIR}/training_curves.png\")\n",
    "\n",
    "print(\"âœ… Setup complete! All imports and model definitions loaded.\")\n",
    "print(f\"âœ… Device: {device}\")\n",
    "print(f\"âœ… Results directory: {RESULTS_DIR}\")\n",
    "print(\"ğŸš€ Ready to run the comparison!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23b87960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Running on GPU.\n"
     ]
    }
   ],
   "source": [
    "#check cuda\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Running on GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298d066b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting LSTM Time Embedding Comparison - FIXED VERSION (STOP ON ERROR)...\n",
      "\n",
      "ğŸ“ Loading datasets...\n",
      "âœ… Dataset loaded: 60000 train, 10000 test samples\n",
      "ğŸ”„ Initializing KAN-MAMMOTE...\n",
      "âœ… KAN-MAMMOTE initialized successfully\n",
      "\n",
      "ğŸ“Š Model Information:\n",
      "   True_Baseline: 200,458 parameters\n",
      "   Learnable_Position: 283,018 parameters\n",
      "   SinCos_LSTM: 232,842 parameters\n",
      "   LETE_LSTM: 252,650 parameters\n",
      "   KAN_MAMMOTE_LSTM: 416,914 parameters\n",
      "\n",
      "==================================================\n",
      "Starting training for True_Baseline\n",
      "==================================================\n",
      "Training True_Baseline...\n",
      "âœ… Dataset loaded: 60000 train, 10000 test samples\n",
      "ğŸ”„ Initializing KAN-MAMMOTE...\n",
      "âœ… KAN-MAMMOTE initialized successfully\n",
      "\n",
      "ğŸ“Š Model Information:\n",
      "   True_Baseline: 200,458 parameters\n",
      "   Learnable_Position: 283,018 parameters\n",
      "   SinCos_LSTM: 232,842 parameters\n",
      "   LETE_LSTM: 252,650 parameters\n",
      "   KAN_MAMMOTE_LSTM: 416,914 parameters\n",
      "\n",
      "==================================================\n",
      "Starting training for True_Baseline\n",
      "==================================================\n",
      "Training True_Baseline...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9e47fc69ce42f68e11ebb21bbbfcb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10 [Train]:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414e66870819497a84d00dfd77aca3d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10 [Test]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Train Acc: 23.24%, Test Acc: 48.50%, Time: 52.6s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdda01e4f559402d898dac14f036cc73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10 [Train]:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e9af9dedcf48d2b1fc219d0f41fc25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10 [Test]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: Train Acc: 62.53%, Test Acc: 72.02%, Time: 52.1s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de3686df39e14c95a79fd451e85dcb11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10 [Train]:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6adc83198d3144159e9d37ed93b9bb34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10 [Test]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: Train Acc: 75.08%, Test Acc: 78.78%, Time: 52.5s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0979087e96774ac4bef995e166a1a7f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10 [Train]:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e2b5fe2d224b9c98ede5282d488451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10 [Test]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: Train Acc: 80.08%, Test Acc: 81.27%, Time: 52.0s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5b564ffa4a4790a84b4fb7d7db3f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10 [Train]:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58acafe5458b4e8e9cf83fd988d0447a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10 [Test]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: Train Acc: 82.80%, Test Acc: 84.32%, Time: 52.4s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0685fbf94414415eb645bc0972c83344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10 [Train]:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad040711b8344d7bafe90c7dcc473b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10 [Test]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: Train Acc: 84.39%, Test Acc: 84.06%, Time: 52.3s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ff5093e8fe47a7ab23dc47ad327137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10 [Train]:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e64d03795ad4e4599ebdb1d4bb40b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10 [Test]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: Train Acc: 85.62%, Test Acc: 85.55%, Time: 52.5s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239b344e78954416a275373f38e4a84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10 [Train]:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f688a74e284eca989f24f955a5df50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10 [Test]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: Train Acc: 86.56%, Test Acc: 87.14%, Time: 52.2s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "881b9af9d112406d8c77d654059ea4a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10 [Train]:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a753e249254451b0729f32e171e68b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10 [Test]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: Train Acc: 87.45%, Test Acc: 86.71%, Time: 52.0s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfbb681727de4424830979d050ea8298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10 [Train]:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a3c5cfb1eb940f69a6d9c132e89fa97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10 [Test]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: Train Acc: 88.19%, Test Acc: 87.47%, Time: 52.1s\n",
      "âœ… True_Baseline training complete. Best test accuracy: 87.47%\n",
      "\n",
      "==================================================\n",
      "Starting training for Learnable_Position\n",
      "==================================================\n",
      "Training Learnable_Position...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5528cb28ef4af4acac1a5cf149c1f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10 [Train]:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5efa8fd81944b79a90c4ff2f067808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10 [Test]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Train Acc: 73.71%, Test Acc: 88.23%, Time: 59.6s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9821ace7e445aa8c68b86d78dea2c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10 [Train]:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d19646c1771491182db63590944ef7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10 [Test]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: Train Acc: 90.95%, Test Acc: 92.85%, Time: 59.1s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a73afb88c0455eb67f260afb453483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10 [Train]:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c9d528f40449f5839ee7f0a3b24a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10 [Test]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: Train Acc: 94.13%, Test Acc: 95.13%, Time: 59.0s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84fd99f44644ed08a22e5e4a278fda1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10 [Train]:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57921d1e9dc44fb69723fb03a154219e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10 [Test]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: Train Acc: 95.65%, Test Acc: 96.40%, Time: 59.0s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a5f0453adb40bb8ddae20f9b7f426f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10 [Train]:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b90acc92918045fab4e7a383247b50d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10 [Test]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: Train Acc: 96.59%, Test Acc: 96.88%, Time: 58.9s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe62202c19634166bd3232711c72c964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10 [Train]:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b22c2705ca43d4a998d04ff2ea4c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10 [Test]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: Train Acc: 97.04%, Test Acc: 96.72%, Time: 58.9s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95de1e3b9f3e431b98cb788494aa78a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10 [Train]:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf74af7a3034e5a8bc5a435bcc3c0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10 [Test]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: Train Acc: 97.54%, Test Acc: 96.95%, Time: 58.9s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6b8f550f21462eaa8d8878a0248396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10 [Train]:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc3e6ef249f4e8f847f502f083489d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10 [Test]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: Train Acc: 97.93%, Test Acc: 96.97%, Time: 59.0s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56514f7b717141a1973e3d79a8ec67d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10 [Train]:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50974431df934ac7a791cf34bd505b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10 [Test]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: Train Acc: 98.11%, Test Acc: 97.23%, Time: 58.9s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8dee50dde140d8aa22af9cbe08221b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10 [Train]:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06911668c6ea431881615707c1a1fe3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10 [Test]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: Train Acc: 98.40%, Test Acc: 97.43%, Time: 58.9s\n",
      "âœ… Learnable_Position training complete. Best test accuracy: 97.43%\n",
      "\n",
      "==================================================\n",
      "Starting training for SinCos_LSTM\n",
      "==================================================\n",
      "Training SinCos_LSTM...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10b658024fc4041b2f4601e7dc5fc51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10 [Train]:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88777b450bd34ed8a852231d5a352709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10 [Test]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Train Acc: 46.92%, Test Acc: 63.36%, Time: 59.0s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e7497a0d814420972c96f7ebecebd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10 [Train]:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e9b4f22b6b4831a96bd4b66a672a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10 [Test]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: Train Acc: 70.17%, Test Acc: 76.49%, Time: 58.8s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4546b4c69b0408bb429d88fdd0d1ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10 [Train]:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15eeec812734414a2a962d0c957ad31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10 [Test]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: Train Acc: 78.10%, Test Acc: 80.62%, Time: 58.8s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d953781b614602b62bd076a95d15f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10 [Train]:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bddcd60b851647fd8399e012c50292aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10 [Test]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: Train Acc: 83.79%, Test Acc: 86.71%, Time: 58.7s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87f6f0530114f179a98d4fd620efac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10 [Train]:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e81b724bb040dfa564de7ec7b6310e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10 [Test]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: Train Acc: 87.53%, Test Acc: 88.46%, Time: 59.1s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a05e6df4f9948dc94299536d8396751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10 [Train]:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function with improved error handling - STOP ON ERROR version.\"\"\"\n",
    "    print(\"ğŸš€ Starting LSTM Time Embedding Comparison - FIXED VERSION (STOP ON ERROR)...\")\n",
    "    \n",
    "    try:\n",
    "        # Create datasets\n",
    "        print(\"\\nğŸ“ Loading datasets...\")\n",
    "        train_dataset = EventBasedMNIST(root='./data', train=True, threshold=THRESHOLD, download=True)\n",
    "        test_dataset = EventBasedMNIST(root='./data', train=False, threshold=THRESHOLD, download=True)\n",
    "        \n",
    "        # FIXED: Validate datasets\n",
    "        if len(train_dataset) == 0 or len(test_dataset) == 0:\n",
    "            raise ValueError(\"Empty dataset!\")\n",
    "        \n",
    "        print(f\"âœ… Dataset loaded: {len(train_dataset)} train, {len(test_dataset)} test samples\")\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ERROR LOADING DATASET - STOPPING: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return  # Stop execution\n",
    "    \n",
    "    # Define models to compare\n",
    "    models = {\n",
    "        'True_Baseline': TrueBaselineLSTM(\n",
    "            input_size=784,\n",
    "            hidden_dim=LSTM_HIDDEN_DIM,\n",
    "            num_classes=10,\n",
    "            dropout=DROPOUT_RATE\n",
    "        ),\n",
    "       'Learnable_Position': LearnablePositionLSTM(\n",
    "            input_size=784,\n",
    "            hidden_dim=LSTM_HIDDEN_DIM,\n",
    "            num_classes=10,\n",
    "            dropout=DROPOUT_RATE\n",
    "        ),\n",
    "        'SinCos_LSTM': SinCosLSTM(\n",
    "            input_size=784,\n",
    "            hidden_dim=LSTM_HIDDEN_DIM,\n",
    "            num_classes=10,\n",
    "            dropout=DROPOUT_RATE\n",
    "        ),\n",
    "        'LETE_LSTM': LETE_LSTM_Fixed(\n",
    "            input_size=784,\n",
    "            hidden_dim=LSTM_HIDDEN_DIM,\n",
    "            num_classes=10,\n",
    "            dropout=DROPOUT_RATE\n",
    "        ),\n",
    "        'KAN_MAMMOTE_LSTM': KAN_MAMMOTE_LSTM_Fixed(\n",
    "            input_size=784,\n",
    "            hidden_dim=LSTM_HIDDEN_DIM,\n",
    "            num_classes=10,\n",
    "            dropout=DROPOUT_RATE\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Print model information\n",
    "    print(\"\\nğŸ“Š Model Information:\")\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            param_count = count_parameters(model)\n",
    "            print(f\"   {name}: {param_count:,} parameters\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ERROR COUNTING PARAMETERS FOR {name} - STOPPING: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return  # Stop execution if model parameter counting fails\n",
    "    \n",
    "    # Train all models\n",
    "    results = {}\n",
    "    best_accuracies = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n\" + \"=\"*50)\n",
    "        print(f\"Starting training for {model_name}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # No try-except here - let errors propagate and stop execution\n",
    "        history, best_acc = train_model(model, train_loader, test_loader, model_name)\n",
    "        results[model_name] = history\n",
    "        best_accuracies[model_name] = best_acc\n",
    "        \n",
    "        # Clean up memory after each model\n",
    "        del model\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Save results\n",
    "    print(\"\\nğŸ’¾ Saving results...\")\n",
    "    \n",
    "    try:\n",
    "        # Save training histories\n",
    "        with open(f\"{RESULTS_DIR}/training_histories.json\", 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        # Save summary results\n",
    "        summary = {\n",
    "            'best_accuracies': best_accuracies,\n",
    "            'model_parameters': {name: count_parameters(models[name]) if name in models else 0 for name in best_accuracies.keys()},\n",
    "            'configuration': {\n",
    "                'batch_size': BATCH_SIZE,\n",
    "                'learning_rate': LEARNING_RATE,\n",
    "                'num_epochs': NUM_EPOCHS,\n",
    "                'lstm_hidden_dim': LSTM_HIDDEN_DIM,\n",
    "                'time_embedding_dim': TIME_EMBEDDING_DIM,\n",
    "                'dropout_rate': DROPOUT_RATE,\n",
    "                'threshold': THRESHOLD\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(f\"{RESULTS_DIR}/summary.json\", 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        # Create CSV summary\n",
    "        with open(f\"{RESULTS_DIR}/results_summary.csv\", 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['Model', 'Best_Accuracy', 'Parameters', 'Avg_Time_per_Epoch'])\n",
    "            \n",
    "            for model_name in best_accuracies.keys():\n",
    "                if model_name in results and len(results[model_name]['training_time']) > 0:\n",
    "                    avg_time = np.mean(results[model_name]['training_time'])\n",
    "                    params = count_parameters(models[model_name])  # Fixed parameter name\n",
    "                    writer.writerow([\n",
    "                        model_name,\n",
    "                        f\"{best_accuracies[model_name]:.4f}\",\n",
    "                        params,\n",
    "                        f\"{avg_time:.2f}\"\n",
    "                    ])\n",
    "        \n",
    "        # Create visualizations\n",
    "        if results:\n",
    "            plot_training_curves(results)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ERROR SAVING RESULTS - STOPPING: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return  # Stop execution\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\nğŸ¯ FINAL RESULTS SUMMARY:\")\n",
    "    print(\"=\" * 80)\n",
    "    for model_name, acc in best_accuracies.items():\n",
    "        params = count_parameters(models[model_name])\n",
    "        if model_name in results and len(results[model_name]['training_time']) > 0:\n",
    "            avg_time = np.mean(results[model_name]['training_time'])\n",
    "            print(f\"{model_name:20s}: {acc:.4f} acc | {params:7,} params | {avg_time:.1f}s/epoch\")\n",
    "    \n",
    "    # Find best model\n",
    "    if best_accuracies and any(acc > 0 for acc in best_accuracies.values()):\n",
    "        best_model = max(best_accuracies, key=best_accuracies.get)\n",
    "        print(f\"\\nğŸ† Best Model: {best_model} (Accuracy: {best_accuracies[best_model]:.4f})\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ All results saved to: {RESULTS_DIR}\")\n",
    "    print(\"ğŸ‰ Comparison complete!\")\n",
    "\n",
    "# Run the comparison\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752ef762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ª Quick Test to Verify Everything Works - STOP ON ERROR\n",
    "print(\"ğŸ§ª Testing the fixed implementation - STOP ON ERROR...\")\n",
    "\n",
    "# Test if all variables are available\n",
    "print(\"âœ… All imports successful\")\n",
    "print(f\"âœ… Device: {device}\")\n",
    "print(f\"âœ… Configuration loaded:\")\n",
    "print(f\"   - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   - Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"   - Time embedding dim: {TIME_EMBEDDING_DIM}\")\n",
    "print(f\"   - LSTM hidden dim: {LSTM_HIDDEN_DIM}\")\n",
    "print(f\"   - Epochs: {NUM_EPOCHS}\")\n",
    "\n",
    "# Test model instantiation\n",
    "print(\"\\nğŸ“Š Testing Model Instantiation:\")\n",
    "# Only create one model at a time to isolate errors\n",
    "print(\"Creating TrueBaselineLSTM...\")\n",
    "true_baseline = TrueBaselineLSTM(hidden_dim=LSTM_HIDDEN_DIM, dropout=DROPOUT_RATE)\n",
    "print(\"âœ… TrueBaselineLSTM created successfully\")\n",
    "\n",
    "print(\"Creating LearnablePositionLSTM...\")\n",
    "learnable_pos = LearnablePositionLSTM(hidden_dim=LSTM_HIDDEN_DIM, dropout=DROPOUT_RATE)\n",
    "print(\"âœ… LearnablePositionLSTM created successfully\")\n",
    "\n",
    "print(\"Creating SinCosLSTM...\")\n",
    "sincos_lstm = SinCosLSTM(hidden_dim=LSTM_HIDDEN_DIM, dropout=DROPOUT_RATE)\n",
    "print(\"âœ… SinCosLSTM created successfully\")\n",
    "\n",
    "print(\"Creating LETE_LSTM_Fixed...\")\n",
    "lete_lstm = LETE_LSTM_Fixed(hidden_dim=LSTM_HIDDEN_DIM, dropout=DROPOUT_RATE)\n",
    "print(\"âœ… LETE_LSTM_Fixed created successfully\")\n",
    "\n",
    "print(\"Creating KAN_MAMMOTE_LSTM_Fixed...\")\n",
    "kan_model = KAN_MAMMOTE_LSTM_Fixed(hidden_dim=LSTM_HIDDEN_DIM, dropout=DROPOUT_RATE)\n",
    "print(\"âœ… KAN_MAMMOTE_LSTM_Fixed created successfully\")\n",
    "\n",
    "# Store models for parameter counting\n",
    "test_models = {\n",
    "    'True_Baseline': true_baseline,\n",
    "    'Learnable_Position': learnable_pos,\n",
    "    'SinCos_LSTM': sincos_lstm,\n",
    "    'LETE_LSTM': lete_lstm,\n",
    "    'KAN_MAMMOTE_LSTM': kan_model\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ“Š Model Parameter Counts:\")\n",
    "for name, model in test_models.items():\n",
    "    params = count_parameters(model)\n",
    "    print(f\"   {name}: {params:,} parameters\")\n",
    "\n",
    "print(\"âœ… All models instantiated successfully\")\n",
    "\n",
    "# Test model internals - KAN-MAMMOTE model\n",
    "print(\"\\nğŸ” Detailed inspection of KAN_MAMMOTE_LSTM model:\")\n",
    "\n",
    "# Print model structure\n",
    "print(\"\\nğŸ“‹ Model Structure:\")\n",
    "print(kan_model)\n",
    "\n",
    "# Check key components\n",
    "print(\"\\nğŸ“Š Key Component Info:\")\n",
    "print(f\"KAN Config: {kan_model.kan_config}\")\n",
    "print(f\"KAN-MAMMOTE initialized: {kan_model.kan_mammote is not None}\")\n",
    "print(f\"Timestamp projector shape: in={kan_model.timestamp_proj.in_features}, out={kan_model.timestamp_proj.out_features}\")\n",
    "print(f\"Feature projector shape: in={kan_model.feature_proj.in_features}, out={kan_model.feature_proj.out_features}\")\n",
    "print(f\"Combiner shape: in={kan_model.combine.in_features}, out={kan_model.combine.out_features}\")\n",
    "print(f\"LSTM input size: {kan_model.lstm.input_size}\")\n",
    "print(f\"LSTM hidden size: {kan_model.lstm.hidden_size}\")\n",
    "print(f\"Classifier shape: in={kan_model.classifier.in_features}, out={kan_model.classifier.out_features}\")\n",
    "\n",
    "# Test dataset creation\n",
    "print(\"\\nğŸ“ Testing Dataset Creation (small sample)...\")\n",
    "# Create a tiny test dataset to verify it works\n",
    "test_dataset = EventBasedMNIST(root='./data', train=True, threshold=THRESHOLD, download=True)\n",
    "print(f\"âœ… Dataset created with {len(test_dataset)} samples\")\n",
    "\n",
    "# Test getting a single item\n",
    "print(\"\\nğŸ“Š Testing single item retrieval:\")\n",
    "events, features, label = test_dataset[0]\n",
    "print(f\"Sample 0 - events shape: {events.shape}, features shape: {features.shape}, label: {label}\")\n",
    "print(f\"Sample 0 - events range: [{events.min()}, {events.max()}]\")\n",
    "print(f\"Sample 0 - features range: [{features.min():.4f}, {features.max():.4f}]\")\n",
    "\n",
    "# Test data loader\n",
    "print(\"\\nğŸ“Š Testing DataLoader:\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "batch = next(iter(test_loader))\n",
    "events, features, lengths, labels = batch\n",
    "\n",
    "print(f\"âœ… DataLoader working:\")\n",
    "print(f\"   - Batch events shape: {events.shape}\")\n",
    "print(f\"   - Batch features shape: {features.shape}\")\n",
    "print(f\"   - Batch lengths: {lengths}\")\n",
    "print(f\"   - Batch labels: {labels}\")\n",
    "\n",
    "# Test data loader with KAN model\n",
    "print(\"\\nğŸ§ª Testing KAN_MAMMOTE_LSTM with a single batch:\")\n",
    "# Move batch to device\n",
    "events = events.to(device)\n",
    "features = features.to(device)\n",
    "lengths = lengths.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "# Create model and move to device\n",
    "test_kan_model = KAN_MAMMOTE_LSTM_Fixed(hidden_dim=LSTM_HIDDEN_DIM, dropout=DROPOUT_RATE).to(device)\n",
    "\n",
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = test_kan_model(events, features, lengths)\n",
    "    \n",
    "print(f\"âœ… Forward pass successful!\")\n",
    "print(f\"   - Output shape: {outputs.shape}\")\n",
    "print(f\"   - Output range: [{outputs.min().item():.4f}, {outputs.max().item():.4f}]\")\n",
    "\n",
    "# Clean up\n",
    "for model in test_models.values():\n",
    "    del model\n",
    "del test_kan_model\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nğŸ‰ Fix verification complete!\")\n",
    "print(\"ğŸš€ You can now run main() in the next cell to start the full comparison!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da00d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ª Testing the fixed KAN_MAMMOTE_LSTM_Fixed model specifically\n",
    "print(\"ğŸ§ª Testing the fixed KAN_MAMMOTE_LSTM_Fixed implementation...\")\n",
    "\n",
    "# Create a small test dataset\n",
    "test_dataset = EventBasedMNIST(root='./data', train=True, threshold=THRESHOLD, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "batch = next(iter(test_loader))\n",
    "events, features, lengths, labels = batch\n",
    "\n",
    "# Move batch to device\n",
    "events = events.to(device)\n",
    "features = features.to(device)\n",
    "lengths = lengths.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "print(f\"\\nğŸ“Š Test batch shapes:\")\n",
    "print(f\"Events: {events.shape}\")\n",
    "print(f\"Features: {features.shape}\")\n",
    "print(f\"Lengths: {lengths}\")\n",
    "print(f\"Labels: {labels}\")\n",
    "\n",
    "# Create model and move to device\n",
    "test_kan_model = KAN_MAMMOTE_LSTM_Fixed(hidden_dim=LSTM_HIDDEN_DIM, dropout=DROPOUT_RATE).to(device)\n",
    "\n",
    "# Test forward pass with detailed error capture\n",
    "try:\n",
    "    print(\"\\nğŸ”„ Running forward pass with fixed model...\")\n",
    "    with torch.no_grad():\n",
    "        outputs = test_kan_model(events, features, lengths)\n",
    "        \n",
    "    print(f\"\\nâœ… Forward pass SUCCESSFUL!\")\n",
    "    print(f\"Output shape: {outputs.shape}\")\n",
    "    print(f\"Output range: [{outputs.min().item():.4f}, {outputs.max().item():.4f}]\")\n",
    "    \n",
    "    # Test backpropagation\n",
    "    print(\"\\nğŸ”„ Testing backpropagation...\")\n",
    "    optimizer = torch.optim.Adam(test_kan_model.parameters(), lr=0.001)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = test_kan_model(events, features, lengths)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"âœ… Backpropagation SUCCESSFUL! Loss: {loss.item():.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ FORWARD PASS FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Clean up\n",
    "del test_kan_model\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nğŸ‰ KAN_MAMMOTE_LSTM_Fixed testing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03c4f36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (kan_mammote)",
   "language": "python",
   "name": "kan_mammote"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
