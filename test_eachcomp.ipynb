{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b833c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'kan-mammote/'\n",
      "/mnt/c/Users/peera/Desktop/kan-mammotev2\n"
     ]
    }
   ],
   "source": [
    "cd kan-mammote/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c03c4f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/kanmote_wsl/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will run on device: cuda\n",
      "\n",
      "KAN-MAMMOTE Model (from src/models/kan_mammote.py) initialized successfully.\n",
      "K-MOTE D_time (output dimension of individual experts): 64\n",
      "K-MOTE K_top (number of active experts chosen by router): 4\n",
      "\n",
      "--- Checking Expert Usage ---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 223\u001b[39m\n\u001b[32m    220\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mK-MOTE K_top (number of active experts chosen by router): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.K_top\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m# 3. Perform the checks and visualizations\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m \u001b[43mcheck_expert_usage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m visualize_expert_functions(model, config)\n\u001b[32m    226\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAll checks complete. Please review \u001b[39m\u001b[33m'\u001b[39m\u001b[33mkmote_expert_usage.png\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mkmote_expert_functions.png\u001b[39m\u001b[33m'\u001b[39m\u001b[33m in your current directory.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mcheck_expert_usage\u001b[39m\u001b[34m(model, config)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Perform a forward pass to collect expert weights for loss\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# KANMAMMOTE.forward returns (model_output, regularization_losses_dict)\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     _, regularization_losses_dict = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimestamps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauxiliary_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# The KANMAMMOTE.forward collects `all_expert_weights_for_loss` and `all_expert_selection_masks`\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# from the K-MOTE instance of the *first* Mamba block (`l_idx == 0`).\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# We retrieve the load_balance_loss, which is derived from the aggregated expert weights.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Let's directly call the K-MOTE of the first block to get a fresh set of masks for plotting.\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# This ensures we get the actual masks, not just the loss component.\u001b[39;00m\n\u001b[32m     67\u001b[39m first_mamba_block = model.mamba_blocks[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/kanmote_wsl/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/kanmote_wsl/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/peera/Desktop/kan-mammotev2/src/models/kan_mammote.py:130\u001b[39m, in \u001b[36mKANMAMMOTE.forward\u001b[39m\u001b[34m(self, timestamps, features, auxiliary_features)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# --- 4. Process Through Stacked ContinuousMambaBlocks ---\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m l_idx, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.mamba_blocks):\n\u001b[32m    128\u001b[39m     \u001b[38;5;66;03m# Call the ContinuousMambaBlock's forward method, passing the current states\u001b[39;00m\n\u001b[32m    129\u001b[39m     \u001b[38;5;66;03m# and receiving the updated states.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     block_output, expert_weights, expert_mask, next_conv_state_layer, next_ssm_state_layer = block(\n\u001b[32m    131\u001b[39m         uk_current_input=current_timestep_input_to_layer, \u001b[38;5;66;03m# Input to this specific block\u001b[39;00m\n\u001b[32m    132\u001b[39m         tk_current=tk_current,\n\u001b[32m    133\u001b[39m         tk_previous=tk_previous,\n\u001b[32m    134\u001b[39m         current_conv_state=current_conv_states[l_idx],   \u001b[38;5;66;03m# State IN for this layer at this timestep\u001b[39;00m\n\u001b[32m    135\u001b[39m         current_ssm_state=current_ssm_states[l_idx]\n\u001b[32m    136\u001b[39m     )\n\u001b[32m    138\u001b[39m     \u001b[38;5;66;03m# The output of the current block becomes the input for the next block in the stack\u001b[39;00m\n\u001b[32m    139\u001b[39m     current_timestep_input_to_layer = block_output\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 5, got 3)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Now, import the modules from your KAN-MAMMOTE project\n",
    "try:\n",
    "    from src.utils.config import KANMAMMOTEConfig\n",
    "    from src.models.kan_mammote import KANMAMMOTE # <<< IMPORTANT: Import the main KANMAMMOTE model\n",
    "    from src.models.k_mote import K_MOTE # For expert names and direct access (though via KANMAMMOTE)\n",
    "    from src.models.kan.MatrixKANLayer import MatrixKANLayer # Explicit import for type checking\n",
    "except ImportError as e:\n",
    "    print(f\"FATAL ERROR: Could not import KAN-MAMMOTE modules. Please check your path setup and file structure.\")\n",
    "    print(f\"Error details: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Configure Matplotlib for saving figures (non-interactive backend)\n",
    "plt.switch_backend('Agg')\n",
    "sns.set_theme(style=\"whitegrid\") # Use seaborn for better aesthetics\n",
    "\n",
    "def check_expert_usage(model: KANMAMMOTE, config: KANMAMMOTEConfig):\n",
    "    \"\"\"\n",
    "    Checks and visualizes the usage frequency of each expert in K-MOTE\n",
    "    within the KANMAMMOTE model.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Checking Expert Usage ---\")\n",
    "\n",
    "    # Generate dummy data for forward pass\n",
    "    batch_size = 64\n",
    "    seq_len = 100\n",
    "    input_feature_dim = config.input_feature_dim\n",
    "    \n",
    "    # Timestamps (e.g., uniformly spaced or random)\n",
    "    timestamps = torch.linspace(0, 10, steps=seq_len).unsqueeze(0).repeat(batch_size, 1) # (B, L)\n",
    "    features = torch.randn(batch_size, seq_len, input_feature_dim) # (B, L, F)\n",
    "    # Auxiliary features are optional, create if config implies their use\n",
    "    auxiliary_features = torch.randn(batch_size, seq_len, config.raw_event_feature_dim) if config.use_aux_features_router and config.raw_event_feature_dim > 0 else None\n",
    "\n",
    "    # Move to device\n",
    "    timestamps = timestamps.to(config.device, dtype=config.dtype)\n",
    "    features = features.to(config.device, dtype=config.dtype)\n",
    "    if auxiliary_features is not None:\n",
    "        auxiliary_features = auxiliary_features.to(config.device, dtype=config.dtype)\n",
    "\n",
    "    # Perform a forward pass to collect expert weights for loss\n",
    "    # KANMAMMOTE.forward returns (model_output, regularization_losses_dict)\n",
    "    with torch.no_grad():\n",
    "        _, regularization_losses_dict = model(timestamps, features, auxiliary_features)\n",
    "\n",
    "    # The KANMAMMOTE.forward collects `all_expert_weights_for_loss` and `all_expert_selection_masks`\n",
    "    # from the K-MOTE instance of the *first* Mamba block (`l_idx == 0`).\n",
    "    # We retrieve the load_balance_loss, which is derived from the aggregated expert weights.\n",
    "    \n",
    "    # To get the raw expert weights for visualization:\n",
    "    # We need to access the K_MOTE router's last computed weights.\n",
    "    # This is slightly tricky because KANMAMMOTE only returns the *loss* values,\n",
    "    # not the raw weights for visualization.\n",
    "    # A cleaner way is to make KANMAMMOTE forward also return expert_selection_mask\n",
    "    # or iterate through the K-MOTE instances in the blocks directly.\n",
    "\n",
    "    # Let's directly call the K-MOTE of the first block to get a fresh set of masks for plotting.\n",
    "    # This ensures we get the actual masks, not just the loss component.\n",
    "    first_mamba_block = model.mamba_blocks[0]\n",
    "    k_mote_instance = first_mamba_block.k_mote\n",
    "    \n",
    "    # Re-run K-MOTE for a sample batch to get expert_selection_mask directly for visualization\n",
    "    # Use the first timestamp and auxiliary feature from the dummy data\n",
    "    sample_tk_current = timestamps[:, 0].unsqueeze(1) # (batch_size, 1)\n",
    "    sample_aux_features = auxiliary_features[:, 0, :] if auxiliary_features is not None else None\n",
    "\n",
    "    _, raw_expert_weights, expert_selection_mask = k_mote_instance(sample_tk_current, sample_aux_features)\n",
    "    \n",
    "    expert_names = k_mote_instance.expert_names\n",
    "    \n",
    "    # Sum selected experts across the batch\n",
    "    # (batch_size, num_experts) -> (num_experts,)\n",
    "    total_selections_per_expert = expert_selection_mask.sum(dim=0).cpu().numpy()\n",
    "    \n",
    "    num_samples_for_viz = batch_size\n",
    "    # Calculate average selection count per sample, then convert to percentage.\n",
    "    # If K_top=1, percentages sum to 100%. If K_top > 1, they sum to K_top * 100%.\n",
    "    usage_percentages = (total_selections_per_expert / num_samples_for_viz) * 100\n",
    "\n",
    "    print(f\"K-MOTE Expert Usage (K_top={config.K_top}, from first block's K-MOTE):\")\n",
    "    for i, name in enumerate(expert_names):\n",
    "        print(f\"- {name}: {usage_percentages[i]:.2f}% selected on average per sample\")\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    sns.barplot(x=expert_names, y=usage_percentages, palette=\"viridis\")\n",
    "    plt.title(f'K-MOTE Expert Usage Percentage (K_top={config.K_top})')\n",
    "    plt.xlabel('Expert Type')\n",
    "    plt.ylabel(f'Average Selection Count (% of samples where expert was chosen)')\n",
    "    # Adjust Y-axis limit to accommodate K_top > 1 scenarios gracefully\n",
    "    plt.ylim(0, config.K_top * 100 + (10 if config.K_top == 1 else 20)) # Add some buffer\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('kmote_expert_usage.png')\n",
    "    print(\"Expert usage plot saved to kmote_expert_usage.png\")\n",
    "    plt.close()\n",
    "\n",
    "def visualize_expert_functions(model: KANMAMMOTE, config: KANMAMMOTEConfig, num_test_points: int = 200):\n",
    "    \"\"\"\n",
    "    Visualizes the functional form of each expert in K-MOTE.\n",
    "    It accesses the internal K_MOTE module from the first Mamba block\n",
    "    and plots the output of each basis function.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Visualizing Individual Expert Functions ---\")\n",
    "\n",
    "    # Access the K_MOTE module from the first ContinuousMambaBlock\n",
    "    first_mamba_block = model.mamba_blocks[0]\n",
    "    k_mote_module: K_MOTE = first_mamba_block.k_mote\n",
    "    expert_names = k_mote_module.expert_names\n",
    "\n",
    "    # Generate a range of timestamp inputs\n",
    "    # A wider range helps see the function's behavior more comprehensively\n",
    "    test_timestamps = torch.linspace(\n",
    "        min(config.kan_grid_range[0], -10.0), # Extend range beyond default if grid_range is small\n",
    "        max(config.kan_grid_range[1], 10.0),\n",
    "        num_test_points\n",
    "    ).unsqueeze(1) # (num_test_points, 1)\n",
    "    test_timestamps = test_timestamps.to(config.device, dtype=config.dtype)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    for name in expert_names:\n",
    "        expert_module = k_mote_module.experts[name]\n",
    "        \n",
    "        with torch.no_grad(): # Disable gradient calculations for visualization\n",
    "            # KANLayer's forward expects (batch_size, in_features) -> (batch_size, out_features)\n",
    "            # Here, in_features is 1 (timestamp), out_features is D_time.\n",
    "            expert_output = expert_module(test_timestamps)\n",
    "            \n",
    "            # MatrixKANLayer returns a tuple (y, preacts, postacts, postspline)\n",
    "            if isinstance(expert_module, MatrixKANLayer):\n",
    "                expert_output = expert_output[0] # Take the main output 'y'\n",
    "\n",
    "            # Each expert outputs a (num_test_points, D_time) tensor.\n",
    "            # To visualize as a single curve, we average across the D_time dimensions.\n",
    "            # This represents the \"average\" function learned by that expert.\n",
    "            visual_output = expert_output.mean(dim=-1).cpu().numpy() # (num_test_points,)\n",
    "\n",
    "        plt.plot(test_timestamps.cpu().numpy().flatten(), visual_output, label=f'{name} Basis', linewidth=2)\n",
    "\n",
    "    plt.title('K-MOTE Individual Expert Basis Functions (Mean across D_time)', fontsize=14)\n",
    "    plt.xlabel('Timestamp Input', fontsize=12)\n",
    "    plt.ylabel('Basis Function Output (Mean)', fontsize=12)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.axhline(0, color='gray', linestyle='-', linewidth=0.8) # Add a horizontal line at y=0\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('kmote_expert_functions.png')\n",
    "    print(\"Individual expert functions plot saved to kmote_expert_functions.png\")\n",
    "    plt.close()\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Configuration for the KAN-MAMMOTE model\n",
    "    # Adjust parameters here to match your model's actual configuration if different\n",
    "    config = KANMAMMOTEConfig(\n",
    "        d_model=128,          # Main model dimension\n",
    "        D_time=64,            # Output dimension of K-MOTE experts\n",
    "        num_layers=2,         # Number of ContinuousMambaBlocks\n",
    "        input_feature_dim=10, # Dummy input feature dimension\n",
    "        output_dim_for_task=1, # Dummy output dimension for the final task\n",
    "        K_top=4,              # Number of top experts selected by MoE router\n",
    "        use_aux_features_router=False, # Set to True if K-MOTE router uses auxiliary_features\n",
    "        raw_event_feature_dim=0, # Must be > 0 if use_aux_features_router is True\n",
    "        \n",
    "        # Mamba2 specific parameters (ensure these are compatible with your Mamba2 setup)\n",
    "        mamba_d_state=64,     # State dimension for Mamba SSM\n",
    "        mamba_headdim=32,     # Dimension per head for Mamba. d_model must be divisible by headdim.\n",
    "        \n",
    "        # KANLayer and Basis Function Parameters\n",
    "        kan_noise_scale=0.1,\n",
    "        kan_grid_range=[-1, 1], # Initial grid range for Spline (MatrixKANLayer)\n",
    "        \n",
    "        # Fourier Basis specific\n",
    "        fourier_k_prime=10,\n",
    "        fourier_learnable_params=True,\n",
    "        \n",
    "        # RKHS / Gaussian Kernel Basis specific\n",
    "        rkhs_num_mixture_components=10,\n",
    "        rkhs_learnable_params=True,\n",
    "        \n",
    "        # Wavelet Basis specific\n",
    "        wavelet_num_wavelets=10,\n",
    "        wavelet_mother_type='mexican_hat', # or 'morlet'\n",
    "        wavelet_learnable_params=True,\n",
    "        \n",
    "        # Spline Basis (MatrixKANLayer) specific\n",
    "        spline_grid_size=8,\n",
    "        spline_degree=3,\n",
    "        kan_sp_trainable=True, # Scale spline trainable\n",
    "        kan_sb_trainable=True, # Scale base trainable\n",
    "\n",
    "        # General training parameters (not directly used in this check script but good to configure)\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=32,\n",
    "        sequence_length=100,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "    \n",
    "    # Validate Mamba headdim and d_model for internal consistency\n",
    "    if config.d_model % config.mamba_headdim != 0:\n",
    "        print(f\"Warning: d_model ({config.d_model}) is not perfectly divisible by mamba_headdim ({config.mamba_headdim}). This might cause issues with Mamba's internal structure.\")\n",
    "\n",
    "    print(f\"Model will run on device: {config.device}\")\n",
    "    \n",
    "    # 2. Instantiate the model\n",
    "    # Instatiate the correct KANMAMMOTE model\n",
    "    model = KANMAMMOTE(config).to(config.device, dtype=config.dtype)\n",
    "    model.eval() # Set to eval mode to disable dropout, router noise, etc., for consistent visualization\n",
    "\n",
    "    print(f\"\\nKAN-MAMMOTE Model (from src/models/kan_mammote.py) initialized successfully.\")\n",
    "    print(f\"K-MOTE D_time (output dimension of individual experts): {config.D_time}\")\n",
    "    print(f\"K-MOTE K_top (number of active experts chosen by router): {config.K_top}\")\n",
    "\n",
    "    # 3. Perform the checks and visualizations\n",
    "    check_expert_usage(model, config)\n",
    "    visualize_expert_functions(model, config)\n",
    "\n",
    "    print(\"\\nAll checks complete. Please review 'kmote_expert_usage.png' and 'kmote_expert_functions.png' in your current directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03f8f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will run on device: cuda\n",
      "\n",
      "--- Capturing Expert Functions at Epoch 0 (Initial State) ---\n",
      "Captured expert functions at Epoch 0.\n",
      "\n",
      "--- Starting Dummy Training for Expert Evolution ---\n",
      "Epoch 1/5, Avg Loss: 10.4433\n",
      "Capturing expert functions after Epoch 1...\n",
      "Snapshot taken for Epoch 1.\n",
      "Epoch 2/5, Avg Loss: 2.3719\n",
      "Capturing expert functions after Epoch 2...\n",
      "Snapshot taken for Epoch 2.\n",
      "Epoch 3/5, Avg Loss: 2.1017\n",
      "Capturing expert functions after Epoch 3...\n",
      "Snapshot taken for Epoch 3.\n",
      "Epoch 4/5, Avg Loss: 2.0524\n",
      "Capturing expert functions after Epoch 4...\n",
      "Snapshot taken for Epoch 4.\n",
      "Epoch 5/5, Avg Loss: 2.0449\n",
      "Capturing expert functions after Epoch 5...\n",
      "Snapshot taken for Epoch 5.\n",
      "\n",
      "--- Dummy Training Complete ---\n",
      "\n",
      "--- Generating Expert Function Evolution Plot ---\n",
      "Expert function evolution plot saved to kmote_expert_functions_evolution.png\n",
      "\n",
      "All visualization complete. Please review 'kmote_expert_functions_evolution.png' in your current directory.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "# Now, import the modules from your KAN-MAMMOTE project\n",
    "try:\n",
    "    from src.utils.config import KANMAMMOTEConfig\n",
    "    from src.models.kan_mammote import KANMAMMOTE # <<< IMPORTANT: Import the main KANMAMMOTE model\n",
    "    from src.models.k_mote import K_MOTE # Used for type hinting and expert access\n",
    "    from src.models.kan.MatrixKANLayer import MatrixKANLayer # Explicit import for type checking\n",
    "except ImportError as e:\n",
    "    print(f\"FATAL ERROR: Could not import KAN-MAMMOTE modules. Please check your path setup and file structure.\")\n",
    "    print(f\"Error details: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Configure Matplotlib for saving figures (non-interactive backend)\n",
    "plt.switch_backend('Agg')\n",
    "sns.set_theme(style=\"whitegrid\") # Use seaborn for better aesthetics\n",
    "\n",
    "def get_expert_output_snapshot(k_mote_instance: K_MOTE, test_timestamps: torch.Tensor) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Helper function to get the current functional output of each K-MOTE expert.\n",
    "    \"\"\"\n",
    "    snapshot = {}\n",
    "    expert_names = k_mote_instance.expert_names\n",
    "    \n",
    "    # Ensure model is in eval mode for consistent visualization (no dropout/noise)\n",
    "    k_mote_instance.eval() \n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculations\n",
    "        for name in expert_names:\n",
    "            expert_module = k_mote_instance.experts[name]\n",
    "            \n",
    "            # KANLayer's forward expects (batch_size, in_features) -> (batch_size, out_features)\n",
    "            # Here, in_features is 1 (timestamp), out_features is D_time.\n",
    "            expert_output = expert_module(test_timestamps)\n",
    "            \n",
    "            # MatrixKANLayer returns a tuple (y, preacts, postacts, postspline)\n",
    "            if isinstance(expert_module, MatrixKANLayer):\n",
    "                expert_output = expert_output[0] # Take the main output 'y'\n",
    "\n",
    "            # Each expert outputs a (num_test_points, D_time) tensor.\n",
    "            # To visualize as a single curve, we average across the D_time dimensions.\n",
    "            snapshot[name] = expert_output.mean(dim=-1).cpu().numpy() # (num_test_points,)\n",
    "    return snapshot\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Configuration for the KAN-MAMMOTE model\n",
    "    config = KANMAMMOTEConfig(\n",
    "        d_model=128,          # Main model dimension\n",
    "        D_time=64,            # Output dimension of K-MOTE experts\n",
    "        num_layers=2,         # Number of ContinuousMambaBlocks\n",
    "        input_feature_dim=10, # Dummy input feature dimension\n",
    "        output_dim_for_task=1, # Dummy output dimension for the final task\n",
    "        K_top=2,              # Number of top experts selected by MoE router\n",
    "        use_aux_features_router=False, # Set to True if K-MOTE router uses auxiliary_features\n",
    "        raw_event_feature_dim=0, # Must be > 0 if use_aux_features_router is True\n",
    "        \n",
    "        # Mamba2 specific parameters\n",
    "        mamba_d_state=64,     # State dimension for Mamba SSM\n",
    "        mamba_headdim=32,     # Dimension per head for Mamba. d_model must be divisible by headdim.\n",
    "        \n",
    "        # KANLayer and Basis Function Parameters (relevant for K-MOTE experts)\n",
    "        kan_noise_scale=0.1,\n",
    "        kan_grid_range=[-1, 1], # Initial grid range for Spline (MatrixKANLayer)\n",
    "        \n",
    "        # Fourier Basis specific\n",
    "        fourier_k_prime=10,\n",
    "        fourier_learnable_params=True,\n",
    "        \n",
    "        # RKHS / Gaussian Kernel Basis specific\n",
    "        rkhs_num_mixture_components=10,\n",
    "        rkhs_learnable_params=True,\n",
    "        \n",
    "        # Wavelet Basis specific\n",
    "        wavelet_num_wavelets=10,\n",
    "        wavelet_mother_type='mexican_hat', # or 'morlet'\n",
    "        wavelet_learnable_params=True,\n",
    "        \n",
    "        # Spline Basis (MatrixKANLayer) specific\n",
    "        spline_grid_size=8,\n",
    "        spline_degree=3,\n",
    "        kan_sp_trainable=True, # Scale spline trainable\n",
    "        kan_sb_trainable=True, # Scale base trainable\n",
    "\n",
    "        # Training parameters for dummy loop\n",
    "        learning_rate=1e-3,\n",
    "        num_epochs=5,         # Total epochs to run (0 to 5 means 6 snapshots)\n",
    "        batch_size=32,\n",
    "        sequence_length=100,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "    \n",
    "    # Validate Mamba headdim and d_model for internal consistency\n",
    "    if config.d_model % config.mamba_headdim != 0:\n",
    "        print(f\"Warning: d_model ({config.d_model}) is not perfectly divisible by mamba_headdim ({config.mamba_headdim}). This might cause issues with Mamba's internal structure.\")\n",
    "\n",
    "    print(f\"Model will run on device: {config.device}\")\n",
    "    \n",
    "    # 2. Instantiate the model, optimizer, and loss\n",
    "    model = KANMAMMOTE(config).to(config.device, dtype=config.dtype)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Define epochs to capture snapshots (0, 1, 2, 3, 4, 5)\n",
    "    epochs_to_capture = [0] + list(range(1, config.num_epochs + 1)) \n",
    "    num_test_points_viz = 200 # Number of points for plotting functions\n",
    "\n",
    "    # Fixed test timestamps for visualization consistency across epochs\n",
    "    test_timestamps_viz = torch.linspace(\n",
    "        min(config.kan_grid_range[0], -10.0), # Extend range beyond default if grid_range is small\n",
    "        max(config.kan_grid_range[1], 10.0),\n",
    "        num_test_points_viz\n",
    "    ).unsqueeze(1).to(config.device, dtype=config.dtype)\n",
    "\n",
    "    expert_function_snapshots = {} # To store {epoch: {expert_name: output_data}}\n",
    "\n",
    "    # Access the K-MOTE instance from the first Mamba block (representative)\n",
    "    # This K_MOTE instance is created inside ContinuousMambaBlock's __init__\n",
    "    k_mote_instance_for_viz = model.mamba_blocks[0].k_mote\n",
    "    expert_names = k_mote_instance_for_viz.expert_names\n",
    "\n",
    "    # --- Capture initial state (Epoch 0) ---\n",
    "    print(\"\\n--- Capturing Expert Functions at Epoch 0 (Initial State) ---\")\n",
    "    expert_function_snapshots[0] = get_expert_output_snapshot(k_mote_instance_for_viz, test_timestamps_viz)\n",
    "    print(\"Captured expert functions at Epoch 0.\")\n",
    "\n",
    "    # --- Dummy Training Loop ---\n",
    "    print(\"\\n--- Starting Dummy Training for Expert Evolution ---\")\n",
    "    num_batches_per_epoch = 10 # Simulate multiple batches per epoch\n",
    "\n",
    "    for epoch in range(1, config.num_epochs + 1):\n",
    "        model.train() # Set model to training mode\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch_idx in range(num_batches_per_epoch):\n",
    "            # Generate new dummy data for each batch\n",
    "            timestamps_batch = torch.linspace(\n",
    "                0, 10, steps=config.sequence_length\n",
    "            ).unsqueeze(0).repeat(config.batch_size, 1).to(config.device, dtype=config.dtype)\n",
    "            features_batch = torch.randn(\n",
    "                config.batch_size, config.sequence_length, config.input_feature_dim\n",
    "            ).to(config.device, dtype=config.dtype)\n",
    "            \n",
    "            auxiliary_features_batch = None\n",
    "            if config.use_aux_features_router and config.raw_event_feature_dim > 0:\n",
    "                auxiliary_features_batch = torch.randn(\n",
    "                    config.batch_size, config.sequence_length, config.raw_event_feature_dim\n",
    "                ).to(config.device, dtype=config.dtype)\n",
    "            \n",
    "            # Dummy target based on timestamps and features (simple regression task)\n",
    "            # Ensure target_output matches the output_dim_for_task\n",
    "            target_output_batch = (\n",
    "                torch.sin(timestamps_batch * 0.5) * 2 + torch.cos(features_batch.mean(dim=-1)) + 5\n",
    "            ).unsqueeze(-1).repeat(1, 1, config.output_dim_for_task).to(config.device, dtype=config.dtype)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            model_output, regularization_losses = model(timestamps_batch, features_batch, auxiliary_features_batch)\n",
    "            \n",
    "            task_loss = criterion(model_output, target_output_batch)\n",
    "            \n",
    "            total_regularization_loss = sum(reg_loss for reg_loss in regularization_losses.values())\n",
    "            \n",
    "            loss = task_loss + total_regularization_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / num_batches_per_epoch\n",
    "        print(f\"Epoch {epoch}/{config.num_epochs}, Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # --- Capture state after this epoch ---\n",
    "        print(f\"Capturing expert functions after Epoch {epoch}...\")\n",
    "        expert_function_snapshots[epoch] = get_expert_output_snapshot(k_mote_instance_for_viz, test_timestamps_viz)\n",
    "        print(f\"Snapshot taken for Epoch {epoch}.\")\n",
    "        \n",
    "    print(\"\\n--- Dummy Training Complete ---\")\n",
    "\n",
    "    # --- Plotting Expert Function Evolution ---\n",
    "    print(\"\\n--- Generating Expert Function Evolution Plot ---\")\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(18, 12))\n",
    "    axes = axes.flatten() # Flatten for easier iteration (ax[0], ax[1], ...)\n",
    "\n",
    "    # Map expert names to subplot indices\n",
    "    expert_map = {name: i for i, name in enumerate(expert_names)}\n",
    "    \n",
    "    # Generate a color palette for epochs\n",
    "    colors = sns.color_palette(\"viridis\", n_colors=len(epochs_to_capture))\n",
    "\n",
    "    for expert_name, ax_idx in expert_map.items():\n",
    "        ax = axes[ax_idx]\n",
    "        ax.set_title(f'{expert_name} Basis Function Evolution', fontsize=14)\n",
    "        ax.set_xlabel('Timestamp Input', fontsize=12)\n",
    "        ax.set_ylabel('Basis Function Output (Mean)', fontsize=12)\n",
    "        ax.grid(True, linestyle='--', alpha=0.7)\n",
    "        ax.axhline(0, color='gray', linestyle='-', linewidth=0.8) # Add a horizontal line at y=0\n",
    "\n",
    "        for i, epoch in enumerate(epochs_to_capture):\n",
    "            ax.plot(test_timestamps_viz.cpu().numpy().flatten(), \n",
    "                    expert_function_snapshots[epoch][expert_name], \n",
    "                    label=f'Epoch {epoch}', \n",
    "                    color=colors[i],\n",
    "                    linewidth=2 if epoch == 0 else 1.5, # Make epoch 0 thicker\n",
    "                    alpha=1.0 if epoch == 0 else 0.8) # Make epoch 0 fully opaque\n",
    "        ax.legend(loc='best', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('kmote_expert_functions_evolution.png')\n",
    "    print(\"Expert function evolution plot saved to kmote_expert_functions_evolution.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # You can re-run expert usage check here if desired, though it's less about\n",
    "    # \"evolution\" and more about overall distribution for a single run.\n",
    "    # check_expert_usage(model, config) \n",
    "\n",
    "    print(\"\\nAll visualization complete. Please review 'kmote_expert_functions_evolution.png' in your current directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a632d910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30d4a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kanmote_wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
