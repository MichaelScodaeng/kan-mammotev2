{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3d0cb26",
   "metadata": {},
   "source": [
    "# MNIST Event-Based Classification with KAN-MOTE\n",
    "\n",
    "This notebook demonstrates how to use KAN-MOTE (Kernel-Mixture-of-Time-Experts) as a time encoder for event-based MNIST classification, replacing the original LeTE (Learnable Time Encoding) approach.\n",
    "\n",
    "## Key Features:\n",
    "- **Adaptive Time Encoding**: KAN-MOTE dynamically selects the best combination of temporal experts\n",
    "- **Multiple Expert Types**: Fourier, Spline, Gaussian Kernel, and Wavelet experts\n",
    "- **Top-K Expert Selection**: Efficient computation by activating only the most relevant experts\n",
    "- **Plug-and-Play**: Direct replacement for LeTE in existing architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87235bb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Add project root to path for imports\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add project root to path for imports\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "# Import KAN-MOTE components\n",
    "from src.models.k_mote import K_MOTE\n",
    "from src.utils.config import KANMAMOTEConfig\n",
    "from src.losses.simple_losses import recommended_feature_loss\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e203fc53",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "We'll convert MNIST images to event sequences by treating non-zero pixels as events, ordered by their positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2b8c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventBasedMNIST(Dataset):\n",
    "    \"\"\"\n",
    "    Converts MNIST images to event sequences.\n",
    "    Each event corresponds to a pixel above the threshold, with timestamp = pixel position.\n",
    "    \"\"\"\n",
    "    def __init__(self, root, train=True, threshold=0.1, transform=None, download=True):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.threshold = threshold\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load MNIST dataset\n",
    "        self.data = datasets.MNIST(root=self.root, train=self.train, \n",
    "                                 transform=self.transform, download=download)\n",
    "        \n",
    "        # Convert images to event sequences\n",
    "        self.event_data = []\n",
    "        self.labels = []\n",
    "        self.process_data()\n",
    "    \n",
    "    def process_data(self):\n",
    "        \"\"\"Convert each MNIST image to an event sequence\"\"\"\n",
    "        for img, label in tqdm(self.data, desc=\"Processing MNIST to events\"):\n",
    "            if isinstance(img, torch.Tensor):\n",
    "                img_flat = img.view(-1)  # (784,)\n",
    "            else:\n",
    "                img_flat = torch.tensor(img).view(-1)\n",
    "            \n",
    "            # Find pixels above threshold (events)\n",
    "            event_indices = torch.nonzero(img_flat > self.threshold).squeeze()\n",
    "            \n",
    "            # Handle edge cases\n",
    "            if event_indices.dim() == 0:\n",
    "                if event_indices.numel() == 1:\n",
    "                    event_indices = event_indices.unsqueeze(0)\n",
    "                else:\n",
    "                    # No events - create a dummy event at position 0\n",
    "                    event_indices = torch.tensor([0])\n",
    "            \n",
    "            # Sort events by position (temporal order)\n",
    "            events = torch.sort(event_indices).values\n",
    "            \n",
    "            self.event_data.append(events)\n",
    "            self.labels.append(label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.event_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.event_data[idx], self.labels[idx]\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"Custom collate function for variable-length event sequences\"\"\"\n",
    "    events_list = []\n",
    "    labels_list = []\n",
    "    lengths = []\n",
    "    \n",
    "    for events, label in batch:\n",
    "        events_list.append(events)\n",
    "        labels_list.append(label)\n",
    "        lengths.append(events.shape[0])\n",
    "    \n",
    "    # Convert to tensors\n",
    "    labels_tensor = torch.tensor(labels_list, dtype=torch.long)\n",
    "    lengths_tensor = torch.tensor(lengths, dtype=torch.long)\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_events = pad_sequence(events_list, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return padded_events, lengths_tensor, labels_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf448363",
   "metadata": {},
   "source": [
    "## KAN-MOTE Configuration\n",
    "\n",
    "Setting up the configuration for KAN-MOTE with appropriate parameters for MNIST event sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0ac40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import KAN-MOTE modules (updated for DyGMamba-style)\n",
    "import sys\n",
    "sys.path.append('.')  # Add project root to path\n",
    "\n",
    "from src.models.k_mote import K_MOTE\n",
    "from src.utils.config import KANMAMOTEConfig\n",
    "from src.losses.simple_losses import recommended_feature_loss\n",
    "\n",
    "# Step 2: Create KAN-MOTE configuration (updated for DyGMamba-style)\n",
    "config = KANMAMOTEConfig(\n",
    "    # Time embedding dimensions\n",
    "    D_time=32,  # Total embedding dimension\n",
    "    D_time_per_expert=8,  # 32/4 = 8 per expert\n",
    "    num_experts=4,  # Fourier, Spline, Gaussian, Wavelet\n",
    "    \n",
    "    # MoE settings\n",
    "    K_top=2,  # Use top-2 experts\n",
    "    use_aux_features_router=False,  # Just timestamps, no auxiliary features\n",
    "    \n",
    "    # Raw event features (in this case, just timestamps)\n",
    "    raw_event_feature_dim=0,  # No additional features beyond timestamps\n",
    "    \n",
    "    # DyGMamba-style Mamba settings\n",
    "    hidden_dim_mamba=32,  # Match output dimension\n",
    "    state_dim_mamba=16,\n",
    "    num_mamba_layers=2,  # Number of Mamba layers\n",
    "    gamma=0.5,  # Time difference scaling factor\n",
    "    \n",
    "    # Regularization (can be adjusted)\n",
    "    lambda_moe_load_balancing=0.01,\n",
    "    lambda_sobolev_l2=0.0,  # Start without additional regularization\n",
    "    lambda_total_variation_l1=0.0\n",
    ")\n",
    "\n",
    "# Step 3: Define KAN-MOTE-based Classifier (simplified, no full KAN-MAMMOTE)\n",
    "class KANMOTELSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size=784, embedding_dim=32, hidden_dim=128, num_classes=10):\n",
    "        super(KANMOTELSTMClassifier, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # KAN-MOTE time encoder (just K-MOTE, not full KAN-MAMMOTE)\n",
    "        self.time_encoder = K_MOTE(config)\n",
    "        \n",
    "        # LSTM and classifier (same as before)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        # Convert pixel indices to timestamps (normalize to [0,1])\n",
    "        timestamps = x.float() / 784.0  # Normalize pixel indices\n",
    "        timestamps = timestamps.unsqueeze(-1)  # (batch, seq_len, 1)\n",
    "        \n",
    "        # Create dummy event features (since we only have timestamps)\n",
    "        batch_size, seq_len = x.shape\n",
    "        event_features = torch.zeros(batch_size, seq_len, 0, device=x.device)  # Empty features\n",
    "        \n",
    "        # Get time embeddings from KAN-MOTE\n",
    "        # K-MOTE expects (batch*seq_len, 1) timestamps\n",
    "        timestamps_flat = timestamps.view(batch_size * seq_len, 1)\n",
    "        event_features_flat = event_features.view(batch_size * seq_len, 0) if event_features.shape[-1] > 0 else None\n",
    "        \n",
    "        # Get embeddings from K-MOTE\n",
    "        time_embeddings, expert_weights, expert_mask = self.time_encoder(timestamps_flat, event_features_flat)\n",
    "        \n",
    "        # Reshape back to sequence format\n",
    "        time_embeddings = time_embeddings.view(batch_size, seq_len, self.embedding_dim)\n",
    "        \n",
    "        # Continue with LSTM processing (same as before)\n",
    "        packed = pack_padded_sequence(time_embeddings, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (h_n, c_n) = self.lstm(packed)\n",
    "        h_n = h_n[-1]  # (batch, hidden_dim)\n",
    "        out = self.fc(h_n)  # (batch, num_classes)\n",
    "        \n",
    "        # Prepare MoE info for regularization\n",
    "        moe_info = {\n",
    "            'expert_weights': expert_weights.view(batch_size, seq_len, -1),\n",
    "            'expert_mask': expert_mask.view(batch_size, seq_len, -1),\n",
    "            'router_logits': expert_weights.view(batch_size, seq_len, -1)\n",
    "        }\n",
    "        \n",
    "        return out, moe_info  # Return MoE info for potential regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd21fa9",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "LSTM classifier using KAN-MOTE for time encoding, replacing the original LeTE approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21450106",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KANMOTELSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM classifier using KAN-MOTE for adaptive time encoding.\n",
    "    \n",
    "    Architecture:\n",
    "    1. KAN-MOTE time encoder: converts pixel positions to rich temporal embeddings\n",
    "    2. LSTM: processes the sequence of time embeddings\n",
    "    3. Classifier: final classification layer\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=784, embedding_dim=32, hidden_dim=128, num_classes=10, config=None):\n",
    "        super(KANMOTELSTMClassifier, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # KAN-MOTE time encoder\n",
    "        self.time_encoder = K_MOTE(config)\n",
    "        \n",
    "        # LSTM for sequence processing\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, \n",
    "                           batch_first=True, dropout=0.1)\n",
    "        \n",
    "        # Classification head\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            x: (batch_size, seq_len) - event positions/timestamps\n",
    "            lengths: (batch_size,) - sequence lengths\n",
    "            \n",
    "        Returns:\n",
    "            logits: (batch_size, num_classes)\n",
    "            moe_info: dictionary with MoE information for regularization\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        # Normalize timestamps to [0, 1] range\n",
    "        timestamps = x.float() / (self.input_size - 1)  # Normalize pixel indices\n",
    "        timestamps = timestamps.view(-1, 1)  # (batch * seq_len, 1)\n",
    "        \n",
    "        # Get time embeddings from KAN-MOTE\n",
    "        time_embeddings, expert_weights, expert_mask = self.time_encoder(\n",
    "            timestamp_input=timestamps, \n",
    "            auxiliary_features=None\n",
    "        )\n",
    "        \n",
    "        # Reshape back to sequence format\n",
    "        time_embeddings = time_embeddings.view(batch_size, seq_len, self.embedding_dim)\n",
    "        \n",
    "        # Process through LSTM\n",
    "        packed = pack_padded_sequence(time_embeddings, lengths.cpu(), \n",
    "                                    batch_first=True, enforce_sorted=False)\n",
    "        _, (h_n, c_n) = self.lstm(packed)\n",
    "        \n",
    "        # Use final hidden state for classification\n",
    "        h_final = h_n[-1]  # (batch_size, hidden_dim)\n",
    "        h_final = self.dropout(h_final)\n",
    "        logits = self.fc(h_final)  # (batch_size, num_classes)\n",
    "        \n",
    "        # Prepare MoE info for potential regularization\n",
    "        moe_info = {\n",
    "            'expert_weights': expert_weights.view(batch_size, seq_len, -1),\n",
    "            'expert_mask': expert_mask.view(batch_size, seq_len, -1),\n",
    "            'router_logits': expert_weights.view(batch_size, seq_len, -1)  # For load balancing\n",
    "        }\n",
    "        \n",
    "        return logits, moe_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc8ee54",
   "metadata": {},
   "source": [
    "## Training Setup\n",
    "\n",
    "Setting up the training pipeline with data loaders, model, and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bcc6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data loading\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = EventBasedMNIST(root=\"./data\", train=True, threshold=0.1, \n",
    "                               transform=transform, download=True)\n",
    "test_dataset = EventBasedMNIST(root=\"./data\", train=False, threshold=0.1, \n",
    "                              transform=transform, download=True)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                         collate_fn=custom_collate_fn, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, \n",
    "                        collate_fn=custom_collate_fn, num_workers=2)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "\n",
    "# Analyze dataset statistics\n",
    "sample_lengths = [len(events) for events, _ in train_dataset]\n",
    "print(f\"Event sequence lengths - Mean: {np.mean(sample_lengths):.1f}, \"\n",
    "      f\"Std: {np.std(sample_lengths):.1f}, \"\n",
    "      f\"Min: {np.min(sample_lengths)}, Max: {np.max(sample_lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bac05b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = KANMOTELSTMClassifier(\n",
    "    input_size=784, \n",
    "    embedding_dim=32, \n",
    "    hidden_dim=128, \n",
    "    num_classes=10,\n",
    "    config=config\n",
    ").to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "# Print model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f43c058",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e7321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device, criterion):\n",
    "    \"\"\"Evaluate model performance on a dataset\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for padded_events, lengths, labels in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            padded_events = padded_events.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs, moe_info = model(padded_events, lengths)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = total_correct / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def analyze_expert_usage(model, data_loader, device, num_batches=10):\n",
    "    \"\"\"Analyze which experts are being used most frequently\"\"\"\n",
    "    model.eval()\n",
    "    expert_usage = torch.zeros(config.num_experts)\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (padded_events, lengths, labels) in enumerate(data_loader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "                \n",
    "            padded_events = padded_events.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            \n",
    "            _, moe_info = model(padded_events, lengths)\n",
    "            expert_mask = moe_info['expert_mask']  # (batch, seq_len, num_experts)\n",
    "            \n",
    "            # Count expert usage\n",
    "            usage = expert_mask.sum(dim=(0, 1)).cpu()  # Sum over batch and sequence\n",
    "            expert_usage += usage\n",
    "            total_tokens += expert_mask.shape[0] * expert_mask.shape[1]\n",
    "    \n",
    "    # Normalize to get percentages\n",
    "    expert_usage_pct = expert_usage / total_tokens * 100\n",
    "    expert_names = ['Fourier', 'Spline', 'Gaussian', 'Wavelet']\n",
    "    \n",
    "    print(\"Expert Usage Analysis:\")\n",
    "    for name, usage in zip(expert_names, expert_usage_pct):\n",
    "        print(f\"  {name}: {usage:.1f}%\")\n",
    "    \n",
    "    return expert_usage_pct.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d38f779",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Training the KAN-MOTE model with regularization and monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80638828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "num_epochs = 50\n",
    "log_file = \"training_log_kan_mote.csv\"\n",
    "results_dir = \"results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Remove existing log file\n",
    "if os.path.exists(log_file):\n",
    "    os.remove(log_file)\n",
    "\n",
    "# Initialize tracking\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "learning_rates = []\n",
    "\n",
    "# CSV logging setup\n",
    "with open(log_file, mode='w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"epoch\", \"train_loss\", \"train_accuracy\", \"test_loss\", \n",
    "                    \"test_accuracy\", \"lr\", \"moe_loss\", \"main_loss\"])\n",
    "\n",
    "print(\"Starting training with KAN-MOTE...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_main_loss = 0.0\n",
    "    total_moe_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # Training loop\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for batch_idx, (padded_events, lengths, labels) in enumerate(progress_bar):\n",
    "        padded_events = padded_events.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, moe_info = model(padded_events, lengths)\n",
    "        \n",
    "        # Main classification loss\n",
    "        main_loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Compute total loss with MoE regularization\n",
    "        total_loss_with_reg, loss_dict = recommended_feature_loss(main_loss, moe_info)\n",
    "        \n",
    "        # Backward pass\n",
    "        total_loss_with_reg.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        total_loss += total_loss_with_reg.item() * labels.size(0)\n",
    "        total_main_loss += main_loss.item() * labels.size(0)\n",
    "        total_moe_loss += (total_loss_with_reg.item() - main_loss.item()) * labels.size(0)\n",
    "        \n",
    "        preds = outputs.argmax(dim=1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        # Update progress bar\n",
    "        current_acc = total_correct / total_samples\n",
    "        progress_bar.set_postfix({\n",
    "            'Loss': f'{total_loss/total_samples:.4f}',\n",
    "            'Acc': f'{current_acc:.4f}'\n",
    "        })\n",
    "    \n",
    "    # Calculate epoch statistics\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_main_loss = total_main_loss / total_samples\n",
    "    avg_moe_loss = total_moe_loss / total_samples\n",
    "    train_acc = total_correct / total_samples\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Evaluation\n",
    "    test_loss, test_acc = evaluate_model(model, test_loader, device, criterion)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(test_loss)\n",
    "    \n",
    "    # Store results\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_acc)\n",
    "    learning_rates.append(current_lr)\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]:\")\n",
    "    print(f\"  Train - Loss: {avg_loss:.4f}, Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Test  - Loss: {test_loss:.4f}, Acc: {test_acc:.4f}\")\n",
    "    print(f\"  MoE Loss: {avg_moe_loss:.6f}, LR: {current_lr:.6f}\")\n",
    "    print()\n",
    "    \n",
    "    # Save to CSV\n",
    "    with open(log_file, mode='a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([epoch+1, avg_loss, train_acc, test_loss, test_acc, \n",
    "                        current_lr, avg_moe_loss, avg_main_loss])\n",
    "    \n",
    "    # Expert usage analysis every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(\"Expert Usage Analysis:\")\n",
    "        analyze_expert_usage(model, test_loader, device, num_batches=5)\n",
    "        print()\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Final Test Accuracy: {test_accuracies[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c617abba",
   "metadata": {},
   "source": [
    "## Results Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67717361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Train Loss', alpha=0.8)\n",
    "plt.plot(test_losses, label='Test Loss', alpha=0.8)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy', alpha=0.8)\n",
    "plt.plot(test_accuracies, label='Test Accuracy', alpha=0.8)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Test Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(learning_rates, alpha=0.8)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_dir}/training_curves_kan_mote.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print final statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best Train Accuracy: {max(train_accuracies):.4f} (Epoch {train_accuracies.index(max(train_accuracies))+1})\")\n",
    "print(f\"Best Test Accuracy:  {max(test_accuracies):.4f} (Epoch {test_accuracies.index(max(test_accuracies))+1})\")\n",
    "print(f\"Final Train Accuracy: {train_accuracies[-1]:.4f}\")\n",
    "print(f\"Final Test Accuracy:  {test_accuracies[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7893796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed expert usage analysis\n",
    "print(\"Detailed Expert Usage Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "expert_usage = analyze_expert_usage(model, test_loader, device, num_batches=20)\n",
    "\n",
    "# Visualize expert usage\n",
    "plt.figure(figsize=(10, 6))\n",
    "expert_names = ['Fourier', 'Spline', 'Gaussian', 'Wavelet']\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "\n",
    "plt.bar(expert_names, expert_usage, color=colors, alpha=0.8, edgecolor='black', linewidth=1)\n",
    "plt.ylabel('Usage Percentage (%)')\n",
    "plt.title('Expert Usage Distribution in KAN-MOTE')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for i, (name, usage) in enumerate(zip(expert_names, expert_usage)):\n",
    "    plt.text(i, usage + 1, f'{usage:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_dir}/expert_usage_kan_mote.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': config,\n",
    "    'train_accuracies': train_accuracies,\n",
    "    'test_accuracies': test_accuracies,\n",
    "    'expert_usage': expert_usage\n",
    "}, f'{results_dir}/kan_mote_mnist_model.pth')\n",
    "\n",
    "print(f\"Model and results saved to {results_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e834924a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates the successful application of KAN-MOTE for event-based MNIST classification. Key observations:\n",
    "\n",
    "### Performance Benefits:\n",
    "- **Adaptive Expert Selection**: KAN-MOTE automatically learns which temporal patterns work best for different timestamps\n",
    "- **Rich Temporal Modeling**: Multiple expert types (Fourier, Spline, Gaussian, Wavelet) capture diverse temporal patterns\n",
    "- **Plug-and-Play**: Direct replacement for LeTE with minimal architecture changes\n",
    "\n",
    "### Expert Usage Insights:\n",
    "The expert usage analysis reveals which temporal patterns are most useful for MNIST event classification, providing interpretability into the model's temporal processing.\n",
    "\n",
    "### Comparison with LeTE:\n",
    "- **LeTE**: Fixed mixing ratio (p=0.5) between Fourier and Spline\n",
    "- **KAN-MOTE**: Dynamic expert selection with 4 expert types and adaptive mixing\n",
    "\n",
    "This demonstrates KAN-MOTE's potential as a superior time encoding method for temporal sequence tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kanmote_wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
