{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66720183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Triton Kernels Available: True\n",
      "✅ All KAN-MAMMOTE modules imported successfully!\n",
      "🔧 Testing on device: cuda with dtype: torch.float32\n",
      "🔧 Using default config: d_model=128, D_time=64, num_layers=2\n"
     ]
    }
   ],
   "source": [
    "# kan_mamote/test_kan_mammote.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "\n",
    "# Suppress all warnings for cleaner output during testing,\n",
    "# but be cautious in production code.\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# --- Import Project Modules ---\n",
    "try:\n",
    "    from src.utils.config import KANMAMMOTEConfig\n",
    "    from src.models.kan_mammote import KANMAMMOTE\n",
    "    from src.models.continuous_mamba_block import ContinuousMambaBlock\n",
    "    from src.models.k_mote import K_MOTE\n",
    "    from src.models.moe_router import MoERouter\n",
    "    from src.layers.basis_functions import FourierBasis, GaussianKernelBasis, WaveletBasis\n",
    "    from src.models.kan.MatrixKANLayer import MatrixKANLayer # For Spline, explicit import\n",
    "    from src.layers.dynamic_mamba_ssm import DynamicMambaSSM # The Mamba core itself\n",
    "    from src.models.regularization import KANMAMMOTE_RegularizationLosses\n",
    "\n",
    "    print(\"✅ All KAN-MAMMOTE modules imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ FATAL ERROR: Failed to import KAN-MAMMOTE modules. Please check your file structure and PYTHONPATH.\")\n",
    "    print(f\"Error details: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- Global Test Configuration ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float32 # Use float32 for most tests, switch to bfloat16/float16 for Mamba if needed\n",
    "\n",
    "# kan_mamote/test_kan_mammote.py (or test_new.ipynb code block)\n",
    "\n",
    "# ... (imports)\n",
    "\n",
    "DEFAULT_CONFIG = KANMAMMOTEConfig(\n",
    "    d_model=128,\n",
    "    D_time=64, # Keep D_time as 64\n",
    "    num_layers=2,\n",
    "    input_feature_dim=10,\n",
    "    output_dim_for_task=1,\n",
    "    K_top=4,\n",
    "    use_aux_features_router=True,\n",
    "    raw_event_feature_dim=5,\n",
    "    # --- FIX for Mamba Stability/Standard Configs ---\n",
    "    mamba_d_state=128,  # A common d_state, often d_model\n",
    "    mamba_d_conv=4,\n",
    "    mamba_expand=2,\n",
    "    mamba_headdim=32,   # nheads = d_ssm / 32\n",
    "    mamba_d_ssm=None,   # Let d_ssm default to d_inner (d_inner = 2*128 = 256)\n",
    "                        # So, d_ssm = 256. nheads = 256 / 32 = 8.\n",
    "                        # dt_modulation_proj will be 64 -> 8.\n",
    "    # ------------------------------------------------\n",
    "    mamba_dt_min=0.001,\n",
    "    mamba_dt_max=0.1,\n",
    "    mamba_dt_init_floor=1e-4,\n",
    "    mamba_bias=False,\n",
    "    mamba_conv_bias=True,\n",
    "    mamba_chunk_size=256,\n",
    "    mamba_use_mem_eff_path=True,\n",
    "    mamba_layer_idx=None,\n",
    "    spline_grid_size=8,\n",
    "    spline_degree=3,\n",
    "    device=DEVICE,\n",
    "    dtype=DTYPE\n",
    ")\n",
    "# ... (rest of the test file)\n",
    "\n",
    "print(f\"🔧 Testing on device: {DEVICE} with dtype: {DTYPE}\")\n",
    "print(f\"🔧 Using default config: d_model={DEFAULT_CONFIG.d_model}, D_time={DEFAULT_CONFIG.D_time}, num_layers={DEFAULT_CONFIG.num_layers}\")\n",
    "\n",
    "# --- Helper Function for Running Tests ---\n",
    "def run_test(name: str, test_func: callable) -> bool:\n",
    "    print(f\"\\n--- Running Test: {name} ---\")\n",
    "    try:\n",
    "        result = test_func()\n",
    "        if result:\n",
    "            print(f\"✅ Test PASSED: {name}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"⚠️ Test COMPLETED WITH WARNINGS: {name}\")\n",
    "            return True # Consider a warning as non-fatal for test suite completion\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Test FAILED: {name}\")\n",
    "        print(f\"   Error: {e}\")\n",
    "        # import traceback\n",
    "        # traceback.print_exc() # Uncomment for full traceback on failure\n",
    "        return False\n",
    "\n",
    "# --- Helper Function for Data Generation ---\n",
    "def create_dummy_data(batch_size: int, seq_len: int, input_dim: int, aux_dim: int = 0) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:\n",
    "    timestamps = torch.randn(batch_size, seq_len, device=DEVICE, dtype=DTYPE) # Use raw random normal\n",
    "    # Apply normalization to timestamps, e.g., to [-1, 1]\n",
    "    # For random normal, [-3, 3] usually covers 99.7% of data.\n",
    "    # Normalize to [-1, 1] range:\n",
    "    timestamps = torch.clamp(timestamps, -3.0, 3.0) / 3.0 # Normalize to approx [-1, 1]\n",
    "    # Or if you want [0,1]:\n",
    "    # timestamps = (timestamps - timestamps.min()) / (timestamps.max() - timestamps.min()) # Min-max over current batch\n",
    "    \n",
    "    # NOTE: For real data, you should calculate min/max over the *entire* dataset and use those fixed values.\n",
    "    # For dummy data, a simple clamp/divide or in-batch min/max is fine for testing.\n",
    "    \n",
    "    features = torch.randn(batch_size, seq_len, input_dim, device=DEVICE, dtype=DTYPE)\n",
    "    \n",
    "    aux_features = None\n",
    "    if aux_dim > 0:\n",
    "        aux_features = torch.randn(batch_size, seq_len, aux_dim, device=DEVICE, dtype=DTYPE)\n",
    "    \n",
    "    return timestamps, features, aux_features\n",
    "\n",
    "# --- Individual Component Test Functions ---\n",
    "\n",
    "def test_01_config_initialization():\n",
    "    \"\"\"Tests if KANMAMMOTEConfig can be initialized and holds expected attributes.\"\"\"\n",
    "    config = KANMAMMOTEConfig(d_model=64, K_top=4, device='cpu')\n",
    "    assert hasattr(config, 'd_model') and config.d_model == 64\n",
    "    assert hasattr(config, 'K_top') and config.K_top == 4\n",
    "    assert hasattr(config, 'device') and config.device == 'cpu'\n",
    "    return True\n",
    "\n",
    "def test_02_kan_mammote_initialization():\n",
    "    \"\"\"Tests if the full KANMAMMOTE model can be initialized.\"\"\"\n",
    "    model = KANMAMMOTE(DEFAULT_CONFIG).to(DEVICE)\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    assert param_count > 0, \"Model should have parameters.\"\n",
    "    assert next(model.parameters()).device == DEVICE, \"Model not on correct device.\"\n",
    "    print(f\"   Model initialized successfully with {param_count:,} parameters.\")\n",
    "    return True\n",
    "\n",
    "def test_03_kan_mammote_forward_pass_full_sequence():\n",
    "    \"\"\"Tests the full KANMAMMOTE forward pass with a sequence of data.\"\"\"\n",
    "    model = KANMAMMOTE(DEFAULT_CONFIG).to(DEVICE)\n",
    "    model.eval() # Set to eval mode\n",
    "\n",
    "    batch_size, seq_len = 4, 16\n",
    "    timestamps, features, aux_features = create_dummy_data(\n",
    "        batch_size, seq_len, DEFAULT_CONFIG.input_feature_dim, DEFAULT_CONFIG.raw_event_feature_dim\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs, regularization_losses = model(timestamps, features, aux_features)\n",
    "\n",
    "    expected_output_shape = (batch_size, seq_len, DEFAULT_CONFIG.output_dim_for_task)\n",
    "    assert outputs.shape == expected_output_shape, \\\n",
    "        f\"Output shape mismatch: Expected {expected_output_shape}, got {outputs.shape}\"\n",
    "    assert not torch.isnan(outputs).any(), \"NaN values detected in model outputs.\"\n",
    "    assert not torch.isinf(outputs).any(), \"Inf values detected in model outputs.\"\n",
    "    assert isinstance(regularization_losses, dict), \"Regularization losses should be a dictionary.\"\n",
    "    assert 'load_balance_loss' in regularization_losses, \"Load balance loss not found.\"\n",
    "    assert regularization_losses['load_balance_loss'].item() >= 0, \"Load balance loss should be non-negative.\"\n",
    "    print(f\"   KANMAMMOTE full forward pass successful. Output shape: {outputs.shape}\")\n",
    "    return True\n",
    "\n",
    "def test_04_kan_mammote_backward_pass_full_sequence():\n",
    "    \"\"\"Tests backward pass for the full KANMAMMOTE model.\"\"\"\n",
    "    model = KANMAMMOTE(DEFAULT_CONFIG).to(DEVICE)\n",
    "    model.train() # Set to training mode\n",
    "\n",
    "    batch_size, seq_len = 4, 16\n",
    "    timestamps, features, aux_features = create_dummy_data(\n",
    "        batch_size, seq_len, DEFAULT_CONFIG.input_feature_dim, DEFAULT_CONFIG.raw_event_feature_dim\n",
    "    )\n",
    "    \n",
    "    # Generate dummy target matching model output shape\n",
    "    # Need a forward pass to get actual output shape first\n",
    "    with torch.no_grad():\n",
    "        sample_outputs, _ = model(timestamps, features, aux_features)\n",
    "    target = torch.randn_like(sample_outputs, device=DEVICE)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs, regularization_losses = model(timestamps, features, aux_features)\n",
    "    task_loss = nn.MSELoss()(outputs, target)\n",
    "    total_loss = task_loss + sum(reg_loss for reg_loss in regularization_losses.values())\n",
    "    total_loss.backward()\n",
    "\n",
    "    # Check for gradients\n",
    "    grads_exist = 0\n",
    "    nan_grads = 0\n",
    "    inf_grads = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            if param.grad is not None:\n",
    "                grads_exist += 1\n",
    "                if torch.isnan(param.grad).any():\n",
    "                    nan_grads += 1\n",
    "                if torch.isinf(param.grad).any():\n",
    "                    inf_grads += 1\n",
    "            else:\n",
    "                print(f\"   Warning: Parameter {name} has requires_grad=True but no grad computed.\")\n",
    "\n",
    "    assert grads_exist > 0, \"No gradients computed for any trainable parameter.\"\n",
    "    assert nan_grads == 0, f\"NaN gradients detected in {nan_grads} parameters.\"\n",
    "    assert inf_grads == 0, f\"Inf gradients detected in {inf_grads} parameters.\"\n",
    "    print(f\"   KANMAMMOTE backward pass successful. Gradients computed for {grads_exist} parameters.\")\n",
    "    return True\n",
    "\n",
    "def test_05_kan_mammote_training_step():\n",
    "    \"\"\"Tests if KANMAMMOTE parameters update after a training step.\"\"\"\n",
    "    model = KANMAMMOTE(DEFAULT_CONFIG).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Store initial parameters\n",
    "    initial_params = [p.clone().detach() for p in model.parameters()]\n",
    "\n",
    "    batch_size, seq_len = 4, 16\n",
    "    timestamps, features, aux_features = create_dummy_data(\n",
    "        batch_size, seq_len, DEFAULT_CONFIG.input_feature_dim, DEFAULT_CONFIG.raw_event_feature_dim\n",
    "    )\n",
    "    \n",
    "    # Generate dummy target\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sample_outputs, _ = model(timestamps, features, aux_features)\n",
    "    target = torch.randn_like(sample_outputs, device=DEVICE)\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs, regularization_losses = model(timestamps, features, aux_features)\n",
    "    task_loss = nn.MSELoss()(outputs, target)\n",
    "    total_loss = task_loss + sum(reg_loss for reg_loss in regularization_losses.values())\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    params_changed = 0\n",
    "    for initial_p, current_p in zip(initial_params, model.parameters()):\n",
    "        if not torch.equal(initial_p, current_p):\n",
    "            params_changed += 1\n",
    "    \n",
    "    assert params_changed > 0, \"No parameters changed after optimizer step.\"\n",
    "    print(f\"   KANMAMMOTE training step successful. {params_changed} parameters changed.\")\n",
    "    return True\n",
    "\n",
    "def test_06_continuous_mamba_block_forward():\n",
    "    \"\"\"Tests a single ContinuousMambaBlock's forward pass including state management.\"\"\"\n",
    "    block = ContinuousMambaBlock(d_model=DEFAULT_CONFIG.d_model, config=DEFAULT_CONFIG).to(DEVICE)\n",
    "    block.eval()\n",
    "\n",
    "    batch_size = 8\n",
    "    # Dummy inputs for a single timestep\n",
    "    uk_current = torch.randn(batch_size, DEFAULT_CONFIG.d_model, device=DEVICE, dtype=DTYPE)\n",
    "    tk_current = torch.randn(batch_size, 1, device=DEVICE, dtype=DTYPE)\n",
    "    tk_previous = torch.randn(batch_size, 1, device=DEVICE, dtype=DTYPE)\n",
    "\n",
    "    # Initialize states (first time step, so all zeros)\n",
    "    # Re-derive state shapes from Mamba2 logic for robustness\n",
    "    d_state = DEFAULT_CONFIG.mamba_d_state\n",
    "    d_conv = DEFAULT_CONFIG.mamba_d_conv\n",
    "    mamba_expand = DEFAULT_CONFIG.mamba_expand\n",
    "    nheads = DEFAULT_CONFIG.d_model // DEFAULT_CONFIG.mamba_headdim # nheads calculation\n",
    "    headdim = DEFAULT_CONFIG.mamba_headdim\n",
    "    d_inner_for_conv_state = DEFAULT_CONFIG.d_model * mamba_expand\n",
    "\n",
    "    current_conv_state = torch.zeros(batch_size, d_inner_for_conv_state, d_conv - 1, device=DEVICE, dtype=DTYPE)\n",
    "    current_ssm_state = torch.zeros(batch_size, nheads, headdim, d_state, device=DEVICE, dtype=DTYPE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hk_current, expert_weights, expert_mask, next_conv_state, next_ssm_state = block(\n",
    "            uk_current, tk_current, tk_previous, current_conv_state, current_ssm_state\n",
    "        )\n",
    "\n",
    "    expected_hk_shape = (batch_size, DEFAULT_CONFIG.d_model)\n",
    "    assert hk_current.shape == expected_hk_shape, f\"hk_current shape mismatch: {hk_current.shape}\"\n",
    "    assert next_conv_state.shape == current_conv_state.shape, \"next_conv_state shape mismatch.\"\n",
    "    assert next_ssm_state.shape == current_ssm_state.shape, \"next_ssm_state shape mismatch.\"\n",
    "    assert not torch.equal(current_conv_state, next_conv_state), \"Conv state did not update.\"\n",
    "    assert not torch.equal(current_ssm_state, next_ssm_state), \"SSM state did not update.\"\n",
    "    \n",
    "    assert not torch.isnan(hk_current).any(), \"NaN in hk_current.\"\n",
    "    assert not torch.isinf(hk_current).any(), \"Inf in hk_current.\"\n",
    "\n",
    "    print(f\"   ContinuousMambaBlock forward successful. hk_current shape: {hk_current.shape}\")\n",
    "    print(f\"   States updated from input states: Conv update detected: {not torch.equal(current_conv_state, next_conv_state)}, SSM update detected: {not torch.equal(current_ssm_state, next_ssm_state)}\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def test_07_k_mote_forward_pass():\n",
    "    \"\"\"Tests K_MOTE's forward pass with and without auxiliary features.\"\"\"\n",
    "    k_mote = K_MOTE(DEFAULT_CONFIG).to(DEVICE)\n",
    "    k_mote.eval()\n",
    "\n",
    "    batch_size = 8\n",
    "    timestamp_input = torch.randn(batch_size, 1, device=DEVICE, dtype=DTYPE)\n",
    "    auxiliary_features = torch.randn(batch_size, DEFAULT_CONFIG.raw_event_feature_dim, device=DEVICE, dtype=DTYPE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Test with auxiliary features\n",
    "        emb_with_aux, weights_with_aux, mask_with_aux = k_mote(timestamp_input, auxiliary_features)\n",
    "        assert emb_with_aux.shape == (batch_size, DEFAULT_CONFIG.D_time), \"K_MOTE output shape mismatch with aux.\"\n",
    "        assert weights_with_aux.shape == (batch_size, 4), \"K_MOTE weights shape mismatch with aux.\"\n",
    "        assert mask_with_aux.shape == (batch_size, 4), \"K_MOTE mask shape mismatch with aux.\"\n",
    "        assert not torch.isnan(emb_with_aux).any(), \"NaN in K_MOTE output with aux.\"\n",
    "\n",
    "        # Test without auxiliary features (pass None)\n",
    "        emb_no_aux, weights_no_aux, mask_no_aux = k_mote(timestamp_input, None)\n",
    "        assert emb_no_aux.shape == (batch_size, DEFAULT_CONFIG.D_time), \"K_MOTE output shape mismatch without aux.\"\n",
    "        print(\"   K_MOTE forward pass successful with and without auxiliary features.\")\n",
    "        return True\n",
    "\n",
    "def test_08_moe_router_forward_pass():\n",
    "    \"\"\"Tests MoERouter's forward pass.\"\"\"\n",
    "    router = MoERouter(input_dim=(1 + DEFAULT_CONFIG.raw_event_feature_dim), num_experts=4, config=DEFAULT_CONFIG).to(DEVICE)\n",
    "    router.eval()\n",
    "\n",
    "    batch_size = 8\n",
    "    timestamp_input = torch.randn(batch_size, 1, device=DEVICE, dtype=DTYPE)\n",
    "    auxiliary_features = torch.randn(batch_size, DEFAULT_CONFIG.raw_event_feature_dim, device=DEVICE, dtype=DTYPE)\n",
    "    router_input = torch.cat([timestamp_input, auxiliary_features], dim=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits, weights = router(timestamp_input, auxiliary_features)\n",
    "    \n",
    "    assert logits.shape == (batch_size, 4), \"Router logits shape mismatch.\"\n",
    "    assert weights.shape == (batch_size, 4), \"Router weights shape mismatch.\"\n",
    "    assert torch.allclose(weights.sum(dim=-1), torch.ones(batch_size, device=DEVICE)), \"Router weights do not sum to 1.\"\n",
    "    assert not torch.isnan(logits).any(), \"NaN in router logits.\"\n",
    "    print(\"   MoERouter forward pass successful.\")\n",
    "    return True\n",
    "\n",
    "def test_09_basis_functions_forward_pass():\n",
    "    \"\"\"Tests forward pass for each individual basis function.\"\"\"\n",
    "    # Note: KANLayer internally transforms the 1D input to D_time dimensions first.\n",
    "    # So we provide (N, D_T) to the BasisFunction, not (N, 1).\n",
    "    # This aligns with how KANLayer's forward calls basis_function.\n",
    "    \n",
    "    batch_size = 8\n",
    "    d_time = DEFAULT_CONFIG.D_time\n",
    "    test_input = torch.randn(batch_size, d_time, device=DEVICE, dtype=DTYPE) # Input to basis function from KANLayer's x_prime\n",
    "\n",
    "    # Fourier Basis\n",
    "    fourier_basis = FourierBasis(d_time, DEFAULT_CONFIG).to(DEVICE)\n",
    "    fourier_output = fourier_basis(test_input)\n",
    "    assert fourier_output.shape == (batch_size, d_time), \"Fourier Basis output shape mismatch.\"\n",
    "    assert not torch.isnan(fourier_output).any(), \"NaN in Fourier Basis output.\"\n",
    "\n",
    "    # Spline Basis (MatrixKANLayer)\n",
    "    # MatrixKANLayer takes (N, in_dim) and returns (y, ...)\n",
    "    spline_basis = MatrixKANLayer(in_dim=1, out_dim=d_time, num=DEFAULT_CONFIG.spline_grid_size, k=DEFAULT_CONFIG.spline_degree, device=DEVICE).to(DEVICE)\n",
    "    # MatrixKANLayer directly expects (N, 1) if its in_dim is 1\n",
    "    # We should test it with what K_MOTE provides to it directly (timestamp_input)\n",
    "    timestamp_input_for_spline = torch.randn(batch_size, 1, device=DEVICE, dtype=DTYPE)\n",
    "    spline_output_tuple = spline_basis(timestamp_input_for_spline) # This returns a tuple\n",
    "    spline_output = spline_output_tuple[0] # The main output\n",
    "    assert spline_output.shape == (batch_size, d_time), \"Spline Basis output shape mismatch.\"\n",
    "    assert not torch.isnan(spline_output).any(), \"NaN in Spline Basis output.\"\n",
    "\n",
    "    # Gaussian Kernel Basis\n",
    "    gaussian_basis = GaussianKernelBasis(d_time, DEFAULT_CONFIG).to(DEVICE)\n",
    "    gaussian_output = gaussian_basis(test_input)\n",
    "    assert gaussian_output.shape == (batch_size, d_time), \"Gaussian Basis output shape mismatch.\"\n",
    "    assert not torch.isnan(gaussian_output).any(), \"NaN in Gaussian Basis output.\"\n",
    "\n",
    "    # Wavelet Basis\n",
    "    wavelet_basis = WaveletBasis(d_time, DEFAULT_CONFIG).to(DEVICE)\n",
    "    wavelet_output = wavelet_basis(test_input)\n",
    "    assert wavelet_output.shape == (batch_size, d_time), \"Wavelet Basis output shape mismatch.\"\n",
    "    assert not torch.isnan(wavelet_output).any(), \"NaN in Wavelet Basis output.\"\n",
    "\n",
    "    print(\"   All Basis Functions forward passes successful.\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def test_10_dynamic_mamba_ssm_step_method():\n",
    "    \"\"\"Tests DynamicMambaSSM's step method for single-token processing with states.\"\"\"\n",
    "    # Mamba2's d_model refers to the overall input/output dimension,\n",
    "    # but its internal calculations for state depend on headdim, d_state, expand.\n",
    "    # The Mamba2 class itself uses d_model as its 'hidden_size'.\n",
    "    \n",
    "    d_model_mamba = DEFAULT_CONFIG.d_model\n",
    "    k_mote_delta_t_embedding_dim = DEFAULT_CONFIG.D_time # The dim of delta_t_embedding\n",
    "\n",
    "    mamba_ssm = DynamicMambaSSM(\n",
    "        d_model=d_model_mamba,\n",
    "        k_mote_delta_t_embedding_dim=k_mote_delta_t_embedding_dim,\n",
    "        d_state=DEFAULT_CONFIG.mamba_d_state,\n",
    "        d_conv=DEFAULT_CONFIG.mamba_d_conv,\n",
    "        expand=DEFAULT_CONFIG.mamba_expand,\n",
    "        headdim=DEFAULT_CONFIG.mamba_headdim,\n",
    "        dt_min=DEFAULT_CONFIG.mamba_dt_min,\n",
    "        dt_max=DEFAULT_CONFIG.mamba_dt_max,\n",
    "        dt_init_floor=DEFAULT_CONFIG.mamba_dt_init_floor,\n",
    "        bias=DEFAULT_CONFIG.mamba_bias,\n",
    "        conv_bias=DEFAULT_CONFIG.mamba_conv_bias,\n",
    "        device=DEVICE,\n",
    "        dtype=DTYPE\n",
    "    ).to(DEVICE)\n",
    "    mamba_ssm.eval()\n",
    "\n",
    "    batch_size = 8\n",
    "    # Input to step method (single token)\n",
    "    hidden_states_step = torch.randn(batch_size, 1, d_model_mamba, device=DEVICE, dtype=DTYPE)\n",
    "    \n",
    "    # dt_modulation_step comes from dt_modulation_proj(delta_t_embedding) (B, nheads)\n",
    "    nheads = d_model_mamba // DEFAULT_CONFIG.mamba_headdim\n",
    "    dt_modulation_step_input = torch.randn(batch_size, nheads, device=DEVICE, dtype=DTYPE)\n",
    "\n",
    "    # Initialize states (zeros for first step)\n",
    "    d_state = DEFAULT_CONFIG.mamba_d_state\n",
    "    d_conv = DEFAULT_CONFIG.mamba_d_conv\n",
    "    mamba_expand = DEFAULT_CONFIG.mamba_expand\n",
    "    d_inner_for_conv_state = d_model_mamba * mamba_expand\n",
    "\n",
    "    conv_state = torch.zeros(batch_size, d_inner_for_conv_state, d_conv - 1, device=DEVICE, dtype=DTYPE)\n",
    "    ssm_state = torch.zeros(batch_size, nheads, DEFAULT_CONFIG.mamba_headdim, d_state, device=DEVICE, dtype=DTYPE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output, next_conv_state, next_ssm_state = mamba_ssm.step(\n",
    "            hidden_states=hidden_states_step,\n",
    "            conv_state=conv_state,\n",
    "            ssm_state=ssm_state,\n",
    "            dt_modulation_step=dt_modulation_step_input\n",
    "        )\n",
    "    \n",
    "    assert output.shape == (batch_size, 1, d_model_mamba), \"DynamicMambaSSM step output shape mismatch.\"\n",
    "    assert next_conv_state.shape == conv_state.shape, \"DynamicMambaSSM next_conv_state shape mismatch.\"\n",
    "    assert next_ssm_state.shape == ssm_state.shape, \"DynamicMambaSSM next_ssm_state shape mismatch.\"\n",
    "    assert not torch.equal(conv_state, next_conv_state), \"Conv state did not update in DynamicMambaSSM.step.\"\n",
    "    assert not torch.equal(ssm_state, next_ssm_state), \"SSM state did not update in DynamicMambaSSM.step.\"\n",
    "\n",
    "    assert not torch.isnan(output).any(), \"NaN in DynamicMambaSSM step output.\"\n",
    "    print(\"   DynamicMambaSSM step method successful with state updates.\")\n",
    "    return True\n",
    "\n",
    "def test_11_regularization_losses():\n",
    "    \"\"\"Tests the regularization loss computations.\"\"\"\n",
    "    reg_handler = KANMAMMOTE_RegularizationLosses(DEFAULT_CONFIG)\n",
    "    \n",
    "    # Test load_balance_loss\n",
    "    batch_size = 16\n",
    "    num_experts = 4 # Hardcoded in K_MOTE\n",
    "    dummy_expert_weights = torch.rand(batch_size, num_experts, device=DEVICE, dtype=DTYPE)\n",
    "    # Make them sum to 1 per row for realistic softmax weights\n",
    "    dummy_expert_weights = dummy_expert_weights / dummy_expert_weights.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    load_loss = reg_handler.compute_load_balance_loss(dummy_expert_weights)\n",
    "    assert isinstance(load_loss, torch.Tensor), \"Load balance loss is not a tensor.\"\n",
    "    assert load_loss.item() >= 0, \"Load balance loss should be non-negative.\"\n",
    "    print(f\"   Load Balance Loss computed: {load_loss.item():.4f}\")\n",
    "\n",
    "    # Test Sobolev and Total Variation (just check if they run without error as they are stubs)\n",
    "    model_for_stub_loss = KANMAMMOTE(DEFAULT_CONFIG).to(DEVICE) # Need a model instance for these stubs\n",
    "    sobolev_loss = reg_handler.compute_sobolev_l2_loss(model_for_stub_loss)\n",
    "    total_variation_loss = reg_handler.compute_total_variation_loss(model_for_stub_loss)\n",
    "    \n",
    "    assert isinstance(sobolev_loss, torch.Tensor) and sobolev_loss.item() == 0.0, \"Sobolev loss stub not working as expected.\"\n",
    "    assert isinstance(total_variation_loss, torch.Tensor) and total_variation_loss.item() == 0.0, \"Total Variation loss stub not working as expected.\"\n",
    "    print(\"   Sobolev L2 and Total Variation loss stubs ran successfully.\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bd459fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting KAN-MAMMOTE Component Tests ---\n",
      "Running tests on device: cuda with dtype: torch.float32\n",
      "Using default config: d_model=128, D_time=64, num_layers=2\n",
      "KANMAMMOTEConfig: <src.utils.config.KANMAMMOTEConfig object at 0x71d355247950>\n",
      "KANMAMMOTEConfig device: cuda, dtype: torch.float32\n",
      "\n",
      "--- Running Test: 01 Config Initialization ---\n",
      "✅ Test PASSED: 01 Config Initialization\n",
      "\n",
      "--- Running Test: 02 Kan Mammote Initialization ---\n",
      "Initializing KAN-MAMMOTE with config: <src.utils.config.KANMAMMOTEConfig object at 0x71d355247950>\n",
      "DEBUG_DMS_INIT: dt_modulation_proj in_features=64, out_features=8 (expected 64 -> 8)\n",
      "DEBUG_DMS_INIT: Calculated self.nheads=8\n",
      "DEBUG_DMS_INIT: dt_modulation_proj in_features=64, out_features=8 (expected 64 -> 8)\n",
      "DEBUG_DMS_INIT: Calculated self.nheads=8\n",
      "KANMAMMOTE init: Pre-calculated conv_channels_for_state=512, nheads_for_state=8\n",
      "   Model initialized successfully with 338,381 parameters on cuda.\n",
      "✅ Test PASSED: 02 Kan Mammote Initialization\n",
      "\n",
      "--- Running Test: 03 Kan Mammote Forward Pass Full Sequence ---\n",
      "Initializing KAN-MAMMOTE with config: <src.utils.config.KANMAMMOTEConfig object at 0x71d355247950>\n",
      "DEBUG_DMS_INIT: dt_modulation_proj in_features=64, out_features=8 (expected 64 -> 8)\n",
      "DEBUG_DMS_INIT: Calculated self.nheads=8\n",
      "DEBUG_DMS_INIT: dt_modulation_proj in_features=64, out_features=8 (expected 64 -> 8)\n",
      "DEBUG_DMS_INIT: Calculated self.nheads=8\n",
      "KANMAMMOTE init: Pre-calculated conv_channels_for_state=512, nheads_for_state=8\n",
      "DEBUG_CMB_FWD_SEQ: delta_t_embedding shape (input to proj): torch.Size([4, 16, 64])\n",
      "DEBUG_CMB_FWD_SEQ: dt_modulation_for_mamba_sequence shape (output of proj): torch.Size([4, 16, 8])\n",
      "DEBUG_DMS_FWD: dt_raw_from_proj shape: torch.Size([4, 16, 8])\n",
      "DEBUG_DMS_FWD: dt_modulation_sequence shape: torch.Size([4, 16, 8])\n",
      "DEBUG_CMB_FWD_SEQ: delta_t_embedding shape (input to proj): torch.Size([4, 16, 64])\n",
      "DEBUG_CMB_FWD_SEQ: dt_modulation_for_mamba_sequence shape (output of proj): torch.Size([4, 16, 8])\n",
      "DEBUG_DMS_FWD: dt_raw_from_proj shape: torch.Size([4, 16, 8])\n",
      "DEBUG_DMS_FWD: dt_modulation_sequence shape: torch.Size([4, 16, 8])\n",
      "   KANMAMMOTE full forward pass successful. Output shape: torch.Size([4, 16, 1])\n",
      "✅ Test PASSED: 03 Kan Mammote Forward Pass Full Sequence\n",
      "\n",
      "--- Running Test: 04 Kan Mammote Backward Pass Full Sequence ---\n",
      "Initializing KAN-MAMMOTE with config: <src.utils.config.KANMAMMOTEConfig object at 0x71d355247950>\n",
      "DEBUG_DMS_INIT: dt_modulation_proj in_features=64, out_features=8 (expected 64 -> 8)\n",
      "DEBUG_DMS_INIT: Calculated self.nheads=8\n",
      "DEBUG_DMS_INIT: dt_modulation_proj in_features=64, out_features=8 (expected 64 -> 8)\n",
      "DEBUG_DMS_INIT: Calculated self.nheads=8\n",
      "KANMAMMOTE init: Pre-calculated conv_channels_for_state=512, nheads_for_state=8\n",
      "DEBUG_CMB_FWD_SEQ: delta_t_embedding shape (input to proj): torch.Size([4, 16, 64])\n",
      "DEBUG_CMB_FWD_SEQ: dt_modulation_for_mamba_sequence shape (output of proj): torch.Size([4, 16, 8])\n",
      "DEBUG_DMS_FWD: dt_raw_from_proj shape: torch.Size([4, 16, 8])\n",
      "DEBUG_DMS_FWD: dt_modulation_sequence shape: torch.Size([4, 16, 8])\n",
      "DEBUG_CMB_FWD_SEQ: delta_t_embedding shape (input to proj): torch.Size([4, 16, 64])\n",
      "DEBUG_CMB_FWD_SEQ: dt_modulation_for_mamba_sequence shape (output of proj): torch.Size([4, 16, 8])\n",
      "DEBUG_DMS_FWD: dt_raw_from_proj shape: torch.Size([4, 16, 8])\n",
      "DEBUG_DMS_FWD: dt_modulation_sequence shape: torch.Size([4, 16, 8])\n",
      "DEBUG_CMB_FWD_SEQ: delta_t_embedding shape (input to proj): torch.Size([4, 16, 64])\n",
      "DEBUG_CMB_FWD_SEQ: dt_modulation_for_mamba_sequence shape (output of proj): torch.Size([4, 16, 8])\n",
      "DEBUG_DMS_FWD: dt_raw_from_proj shape: torch.Size([4, 16, 8])\n",
      "DEBUG_DMS_FWD: dt_modulation_sequence shape: torch.Size([4, 16, 8])\n",
      "DEBUG_CMB_FWD_SEQ: delta_t_embedding shape (input to proj): torch.Size([4, 16, 64])\n",
      "DEBUG_CMB_FWD_SEQ: dt_modulation_for_mamba_sequence shape (output of proj): torch.Size([4, 16, 8])\n",
      "DEBUG_DMS_FWD: dt_raw_from_proj shape: torch.Size([4, 16, 8])\n",
      "DEBUG_DMS_FWD: dt_modulation_sequence shape: torch.Size([4, 16, 8])\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.router.router_network.0.weight has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.router.router_network.0.bias has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.router.router_network.2.weight has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.router.router_network.2.bias has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.router.router_network.4.weight has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.router.router_network.4.bias has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.experts.fourier.alpha_weights has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.experts.fourier.alpha_bias has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.experts.fourier.norm_after_linear.weight has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.experts.fourier.norm_after_linear.bias has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.experts.fourier.basis_function.frequencies has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.experts.fourier.basis_function.amplitudes has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.experts.fourier.basis_function.phases has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.experts.spline.coef has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.experts.spline.scale_base has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.experts.spline.scale_sp has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.experts.gaussian.alpha_weights has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.experts.gaussian.alpha_bias has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.experts.gaussian.norm_after_linear.weight has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.experts.gaussian.norm_after_linear.bias has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.experts.gaussian.basis_function.raw_weights has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.experts.gaussian.basis_function.means has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.experts.gaussian.basis_function.raw_stds has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.experts.wavelet.alpha_weights has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.experts.wavelet.alpha_bias has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.experts.wavelet.norm_after_linear.weight has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.experts.wavelet.norm_after_linear.bias has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.experts.wavelet.basis_function.weights has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.experts.wavelet.basis_function.raw_scales has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.experts.wavelet.basis_function.translations has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.layer_norm.weight has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.k_mote.layer_norm.bias has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.faster_kan_transform.kan_transform.alpha_weights has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.faster_kan_transform.kan_transform.alpha_bias has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.faster_kan_transform.kan_transform.norm_after_linear.weight has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.faster_kan_transform.kan_transform.norm_after_linear.bias has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.faster_kan_transform.kan_transform.basis_function.raw_weights has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.faster_kan_transform.kan_transform.basis_function.means has requires_grad=True but no grad computed.\n",
      "   Warning: Parameter mamba_blocks.1.faster_kan_transform.kan_transform.basis_function.raw_stds has requires_grad=True but no grad computed.\n",
      "   KANMAMMOTE backward pass successful. Gradients computed for 67 parameters.\n",
      "✅ Test PASSED: 04 Kan Mammote Backward Pass Full Sequence\n",
      "\n",
      "--- Running Test: 05 Kan Mammote Training Step ---\n",
      "Initializing KAN-MAMMOTE with config: <src.utils.config.KANMAMMOTEConfig object at 0x71d355247950>\n",
      "DEBUG_DMS_INIT: dt_modulation_proj in_features=64, out_features=8 (expected 64 -> 8)\n",
      "DEBUG_DMS_INIT: Calculated self.nheads=8\n",
      "DEBUG_DMS_INIT: dt_modulation_proj in_features=64, out_features=8 (expected 64 -> 8)\n",
      "DEBUG_DMS_INIT: Calculated self.nheads=8\n",
      "KANMAMMOTE init: Pre-calculated conv_channels_for_state=512, nheads_for_state=8\n",
      "DEBUG_CMB_FWD_SEQ: delta_t_embedding shape (input to proj): torch.Size([4, 16, 64])\n",
      "DEBUG_CMB_FWD_SEQ: dt_modulation_for_mamba_sequence shape (output of proj): torch.Size([4, 16, 8])\n",
      "DEBUG_DMS_FWD: dt_raw_from_proj shape: torch.Size([4, 16, 8])\n",
      "DEBUG_DMS_FWD: dt_modulation_sequence shape: torch.Size([4, 16, 8])\n",
      "DEBUG_CMB_FWD_SEQ: delta_t_embedding shape (input to proj): torch.Size([4, 16, 64])\n",
      "DEBUG_CMB_FWD_SEQ: dt_modulation_for_mamba_sequence shape (output of proj): torch.Size([4, 16, 8])\n",
      "DEBUG_DMS_FWD: dt_raw_from_proj shape: torch.Size([4, 16, 8])\n",
      "DEBUG_DMS_FWD: dt_modulation_sequence shape: torch.Size([4, 16, 8])\n",
      "DEBUG_CMB_FWD_SEQ: delta_t_embedding shape (input to proj): torch.Size([4, 16, 64])\n",
      "DEBUG_CMB_FWD_SEQ: dt_modulation_for_mamba_sequence shape (output of proj): torch.Size([4, 16, 8])\n",
      "DEBUG_DMS_FWD: dt_raw_from_proj shape: torch.Size([4, 16, 8])\n",
      "DEBUG_DMS_FWD: dt_modulation_sequence shape: torch.Size([4, 16, 8])\n",
      "DEBUG_CMB_FWD_SEQ: delta_t_embedding shape (input to proj): torch.Size([4, 16, 64])\n",
      "DEBUG_CMB_FWD_SEQ: dt_modulation_for_mamba_sequence shape (output of proj): torch.Size([4, 16, 8])\n",
      "DEBUG_DMS_FWD: dt_raw_from_proj shape: torch.Size([4, 16, 8])\n",
      "DEBUG_DMS_FWD: dt_modulation_sequence shape: torch.Size([4, 16, 8])\n",
      "   KANMAMMOTE training step successful. 34 parameters changed.\n",
      "✅ Test PASSED: 05 Kan Mammote Training Step\n",
      "\n",
      "--- Running Test: 06 Continuous Mamba Block Forward ---\n",
      "DEBUG_DMS_INIT: dt_modulation_proj in_features=64, out_features=8 (expected 64 -> 8)\n",
      "DEBUG_DMS_INIT: Calculated self.nheads=8\n",
      "DEBUG_CMB_FWD_SEQ: delta_t_embedding shape (input to proj): torch.Size([8, 16, 64])\n",
      "DEBUG_CMB_FWD_SEQ: dt_modulation_for_mamba_sequence shape (output of proj): torch.Size([8, 16, 8])\n",
      "DEBUG_DMS_FWD: dt_raw_from_proj shape: torch.Size([8, 16, 8])\n",
      "DEBUG_DMS_FWD: dt_modulation_sequence shape: torch.Size([8, 16, 8])\n",
      "   ContinuousMambaBlock forward_sequence successful. Output shape: torch.Size([8, 16, 128])\n",
      "✅ Test PASSED: 06 Continuous Mamba Block Forward\n",
      "\n",
      "--- Running Test: 07 K Mote Forward Pass ---\n",
      "   K_MOTE forward pass successful with and without auxiliary features.\n",
      "✅ Test PASSED: 07 K Mote Forward Pass\n",
      "\n",
      "--- Running Test: 08 Moe Router Forward Pass ---\n",
      "   MoERouter forward pass successful.\n",
      "✅ Test PASSED: 08 Moe Router Forward Pass\n",
      "\n",
      "--- Running Test: 09 Basis Functions Forward Pass ---\n",
      "   All Basis Functions forward passes successful.\n",
      "✅ Test PASSED: 09 Basis Functions Forward Pass\n",
      "\n",
      "--- Running Test: 10 Dynamic Mamba Ssm Step Method ---\n",
      "\n",
      "=== DynamicMambaSSM Step Method Test ===\n",
      "-> Initializing DynamicMambaSSM module...\n",
      "DEBUG_DMS_INIT: dt_modulation_proj in_features=64, out_features=8 (expected 64 -> 8)\n",
      "DEBUG_DMS_INIT: Calculated self.nheads=8\n",
      "-> Generating input tensors...\n",
      "   [Input] hidden_states_step shape: torch.Size([8, 1, 128]), min: -3.1295, max: 3.3696\n",
      "   [Input] dt_modulation_step_input shape: torch.Size([8, 8])\n",
      "   [State] conv_state shape: torch.Size([8, 512, 3]), sum: 0.0000\n",
      "   [State] ssm_state shape: torch.Size([8, 8, 32, 128]), sum: 0.0000\n",
      "-> Running DynamicMambaSSM.step()...\n",
      "DEBUG_DMS_STEP_SPLIT: z0 shape: torch.Size([8, 0])\n",
      "DEBUG_DMS_STEP_SPLIT: x0 shape: torch.Size([8, 0])\n",
      "DEBUG_DMS_STEP_SPLIT: z shape: torch.Size([8, 256])\n",
      "DEBUG_DMS_STEP_SPLIT: xBC_for_conv shape: torch.Size([8, 512])\n",
      "DEBUG_DMS_STEP_SPLIT: dt_raw_from_proj shape: torch.Size([8, 8])\n",
      "   [Output] output shape: torch.Size([8, 1, 128]), min: -1.7437, max: 1.6944, abs_sum: 485.6382\n",
      "   [State] next_conv_state shape: torch.Size([8, 512, 3]), changed: True\n",
      "   [State] next_ssm_state shape: torch.Size([8, 8, 32, 128]), changed: True\n",
      "✅ DynamicMambaSSM step method successful with state updates and valid output.\n",
      "✅ Test PASSED: 10 Dynamic Mamba Ssm Step Method\n",
      "\n",
      "--- All Component Tests Finished ---\n",
      "✅ CONGRATULATIONS! All KAN-MAMMOTE components appear to be working correctly.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import traceback\n",
    "# --- Helper Function for Running Tests ---\n",
    "def run_test(name: str, test_func: callable) -> bool:\n",
    "    print(f\"\\n--- Running Test: {name} ---\")\n",
    "    try:\n",
    "        result = test_func()\n",
    "        if result:\n",
    "            print(f\"✅ Test PASSED: {name}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"⚠️ Test COMPLETED WITH WARNINGS: {name}\")\n",
    "            return True # Consider a warning as non-fatal for test suite completion\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Test FAILED: {name}\")\n",
    "        print(f\"   Error: {e}\")\n",
    "        # import traceback\n",
    "        # traceback.print_exc() # Uncomment for full traceback on failure\n",
    "        return False\n",
    "\n",
    "# --- Helper Function for Data Generation ---\n",
    "def create_dummy_data(batch_size: int, seq_len: int, input_dim: int, aux_dim: int = 0) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:\n",
    "    timestamps = torch.randn(batch_size, seq_len, device=DEVICE, dtype=DTYPE) * 10.0 # Random timestamps\n",
    "    features = torch.randn(batch_size, seq_len, input_dim, device=DEVICE, dtype=DTYPE)\n",
    "    \n",
    "    aux_features = None\n",
    "    if aux_dim > 0:\n",
    "        aux_features = torch.randn(batch_size, seq_len, aux_dim, device=DEVICE, dtype=DTYPE)\n",
    "    \n",
    "    return timestamps, features, aux_features\n",
    "\n",
    "# --- Individual Component Test Functions ---\n",
    "\n",
    "def test_01_config_initialization():\n",
    "    \"\"\"Tests if KANMAMMOTEConfig can be initialized and holds expected attributes.\"\"\"\n",
    "    config = KANMAMMOTEConfig(d_model=64, K_top=4, device='cpu')\n",
    "    assert hasattr(config, 'd_model') and config.d_model == 64\n",
    "    assert hasattr(config, 'K_top') and config.K_top == 4\n",
    "    assert hasattr(config, 'device') and config.device == 'cpu'\n",
    "    return True\n",
    "\n",
    "def test_02_kan_mammote_initialization():\n",
    "    \"\"\"Tests if the full KANMAMMOTE model can be initialized and is on the correct device.\"\"\"\n",
    "    config = DEFAULT_CONFIG\n",
    "    \n",
    "    model = KANMAMMOTE(config).to(DEVICE) # Ensure model is moved right after creation\n",
    "\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    assert param_count > 0, \"Model should have parameters.\"\n",
    "\n",
    "    # FIX for 'not on correct device. Expected cuda, got cuda:0'\n",
    "    for name, param in model.named_parameters():\n",
    "        assert param.device.type == DEVICE and \\\n",
    "               (param.device.index == 0 if DEVICE == 'cuda' else True), \\\n",
    "               f\"Parameter '{name}' (shape {param.shape}) not on correct device. Expected {DEVICE}, got {param.device}\"\n",
    "\n",
    "    print(f\"   Model initialized successfully with {param_count:,} parameters on {DEVICE}.\")\n",
    "    return True\n",
    "\n",
    "# Helper to create dummy data needs to support auxiliary features\n",
    "def create_dummy_data(batch_size: int, seq_len: int, input_dim: int, aux_dim: int = 0) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:\n",
    "    timestamps = torch.randn(batch_size, seq_len, device=DEVICE, dtype=DTYPE) * 10.0\n",
    "    features = torch.randn(batch_size, seq_len, input_dim, device=DEVICE, dtype=DTYPE)\n",
    "    \n",
    "    aux_features = None\n",
    "    if aux_dim > 0: # Only create if aux_dim is specified\n",
    "        aux_features = torch.randn(batch_size, seq_len, aux_dim, device=DEVICE, dtype=DTYPE)\n",
    "    \n",
    "    return timestamps, features, aux_features\n",
    "\n",
    "def test_03_kan_mammote_forward_pass_full_sequence():\n",
    "    \"\"\"Tests the full KANMAMMOTE forward pass with a sequence of data.\"\"\"\n",
    "    model = KANMAMMOTE(DEFAULT_CONFIG).to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    batch_size, seq_len = 4, 16\n",
    "    timestamps, features, aux_features = create_dummy_data(\n",
    "        batch_size, seq_len, DEFAULT_CONFIG.input_feature_dim, DEFAULT_CONFIG.raw_event_feature_dim\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outputs, regularization_losses = model(timestamps, features, aux_features)\n",
    "\n",
    "        expected_output_shape = (batch_size, seq_len, DEFAULT_CONFIG.output_dim_for_task)\n",
    "        assert outputs.shape == expected_output_shape, \\\n",
    "            f\"Output shape mismatch: Expected {expected_output_shape}, got {outputs.shape}\"\n",
    "        assert not torch.isnan(outputs).any(), \"NaN values detected in model outputs.\"\n",
    "        assert not torch.isinf(outputs).any(), \"Inf values detected in model outputs.\"\n",
    "        assert isinstance(regularization_losses, dict), \"Regularization losses should be a dictionary.\"\n",
    "        assert 'load_balance_loss' in regularization_losses, \"Load balance loss not found.\"\n",
    "        assert regularization_losses['load_balance_loss'].item() >= 0, \"Load balance loss should be non-negative.\"\n",
    "        print(f\"   KANMAMMOTE full forward pass successful. Output shape: {outputs.shape}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Test FAILED: 03 Kan Mammote Forward Pass Full Sequence\")\n",
    "        print(f\"   Error: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def test_04_kan_mammote_backward_pass_full_sequence():\n",
    "    \"\"\"Tests backward pass for the full KANMAMMOTE model.\"\"\"\n",
    "    model = KANMAMMOTE(DEFAULT_CONFIG).to(DEVICE)\n",
    "    model.train()\n",
    "\n",
    "    batch_size, seq_len = 4, 16\n",
    "    # FIX: Ensure aux_features are generated and passed\n",
    "    timestamps, features, aux_features = create_dummy_data(\n",
    "        batch_size, seq_len, DEFAULT_CONFIG.input_feature_dim, DEFAULT_CONFIG.raw_event_feature_dim\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sample_outputs, _ = model(timestamps, features, aux_features) # Pass aux_features\n",
    "    target = torch.randn_like(sample_outputs, device=DEVICE)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs, regularization_losses = model(timestamps, features, aux_features) # Pass aux_features\n",
    "    task_loss = nn.MSELoss()(outputs, target)\n",
    "    total_loss = task_loss + sum(reg_loss for reg_loss in regularization_losses.values())\n",
    "    total_loss.backward()\n",
    "\n",
    "    grads_exist = 0\n",
    "    nan_grads = 0\n",
    "    inf_grads = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            if param.grad is not None:\n",
    "                grads_exist += 1\n",
    "                if torch.isnan(param.grad).any():\n",
    "                    nan_grads += 1\n",
    "                if torch.isinf(param.grad).any():\n",
    "                    inf_grads += 1\n",
    "            else:\n",
    "                print(f\"   Warning: Parameter {name} has requires_grad=True but no grad computed.\")\n",
    "\n",
    "    assert grads_exist > 0, \"No gradients computed for any trainable parameter.\"\n",
    "    assert nan_grads == 0, f\"NaN gradients detected in {nan_grads} parameters.\"\n",
    "    assert inf_grads == 0, f\"Inf gradients detected in {inf_grads} parameters.\"\n",
    "    print(f\"   KANMAMMOTE backward pass successful. Gradients computed for {grads_exist} parameters.\")\n",
    "    return True\n",
    "\n",
    "def test_05_kan_mammote_training_step():\n",
    "    \"\"\"Tests if KANMAMMOTE parameters update after a training step.\"\"\"\n",
    "    model = KANMAMMOTE(DEFAULT_CONFIG).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    initial_params = [p.clone().detach() for p in model.parameters()]\n",
    "\n",
    "    batch_size, seq_len = 4, 16\n",
    "    # FIX: Ensure aux_features are generated and passed\n",
    "    timestamps, features, aux_features = create_dummy_data(\n",
    "        batch_size, seq_len, DEFAULT_CONFIG.input_feature_dim, DEFAULT_CONFIG.raw_event_feature_dim\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sample_outputs, _ = model(timestamps, features, aux_features) # Pass aux_features\n",
    "    target = torch.randn_like(sample_outputs, device=DEVICE)\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs, regularization_losses = model(timestamps, features, aux_features) # Pass aux_features\n",
    "    task_loss = nn.MSELoss()(outputs, target)\n",
    "    total_loss = task_loss + sum(reg_loss for reg_loss in regularization_losses.values())\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    params_changed = 0\n",
    "    for initial_p, current_p in zip(initial_params, model.parameters()):\n",
    "        if not torch.equal(initial_p, current_p):\n",
    "            params_changed += 1\n",
    "    \n",
    "    assert params_changed > 0, \"No parameters changed after optimizer step.\"\n",
    "    print(f\"   KANMAMMOTE training step successful. {params_changed} parameters changed.\")\n",
    "    return True\n",
    "\n",
    "def test_06_continuous_mamba_block_forward():\n",
    "    \"\"\"Tests a single ContinuousMambaBlock's forward_sequence pass.\"\"\"\n",
    "    block = ContinuousMambaBlock(d_model=DEFAULT_CONFIG.d_model, config=DEFAULT_CONFIG).to(DEVICE)\n",
    "    block.eval()\n",
    "\n",
    "    batch_size, seq_len = 8, 16 # Test with a sequence\n",
    "    \n",
    "    # Inputs for forward_sequence: hidden_states (u) and delta_t_embedding\n",
    "    hidden_states_input = torch.randn(batch_size, seq_len, DEFAULT_CONFIG.d_model, device=DEVICE, dtype=DTYPE)\n",
    "    \n",
    "    # Generate a dummy delta_t_embedding. Its last dim should be D_time (64).\n",
    "    dummy_delta_t_embedding = torch.randn(batch_size, seq_len, DEFAULT_CONFIG.D_time, device=DEVICE, dtype=DTYPE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # FIX: Call forward_sequence directly, passing the correct arguments\n",
    "        output_sequence = block.forward_sequence(\n",
    "            hidden_states=hidden_states_input,\n",
    "            delta_t_embedding=dummy_delta_t_embedding\n",
    "        )\n",
    "    \n",
    "    expected_output_shape = (batch_size, seq_len, DEFAULT_CONFIG.d_model)\n",
    "    assert output_sequence.shape == expected_output_shape, f\"ContinuousMambaBlock forward_sequence output shape mismatch: {output_sequence.shape}\"\n",
    "    assert not torch.isnan(output_sequence).any(), \"NaN in ContinuousMambaBlock forward_sequence output.\"\n",
    "    assert not torch.isinf(output_sequence).any(), \"Inf in ContinuousMambaBlock forward_sequence output.\"\n",
    "\n",
    "    print(f\"   ContinuousMambaBlock forward_sequence successful. Output shape: {output_sequence.shape}\")\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_07_k_mote_forward_pass():\n",
    "    \"\"\"Tests K_MOTE's forward pass with and without auxiliary features.\"\"\"\n",
    "    k_mote = K_MOTE(DEFAULT_CONFIG).to(DEVICE)\n",
    "    k_mote.eval()\n",
    "\n",
    "    batch_size = 8\n",
    "    timestamp_input = torch.randn(batch_size, 1, device=DEVICE, dtype=DTYPE)\n",
    "    auxiliary_features_for_k_mote = torch.randn(batch_size, DEFAULT_CONFIG.raw_event_feature_dim, device=DEVICE, dtype=DTYPE) # FIX: Use a specific variable name for clarity\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Test with auxiliary features (this now should work correctly)\n",
    "        emb_with_aux, weights_with_aux, mask_with_aux = k_mote(timestamp_input, auxiliary_features_for_k_mote)\n",
    "        assert emb_with_aux.shape == (batch_size, DEFAULT_CONFIG.D_time), \"K_MOTE output shape mismatch with aux.\"\n",
    "        assert weights_with_aux.shape == (batch_size, 4), \"K_MOTE weights shape mismatch with aux.\"\n",
    "        assert mask_with_aux.shape == (batch_size, 4), \"K_MOTE mask shape mismatch with aux.\"\n",
    "        assert not torch.isnan(emb_with_aux).any(), \"NaN in K_MOTE output with aux.\"\n",
    "\n",
    "        # Test without auxiliary features (pass None)\n",
    "        # Ensure DEFAULT_CONFIG.use_aux_features_router is respected by K_MOTE's internal logic\n",
    "        # (router will just use timestamp_input)\n",
    "        emb_no_aux, weights_no_aux, mask_no_aux = k_mote(timestamp_input, None)\n",
    "        assert emb_no_aux.shape == (batch_size, DEFAULT_CONFIG.D_time), \"K_MOTE output shape mismatch without aux.\"\n",
    "        print(\"   K_MOTE forward pass successful with and without auxiliary features.\")\n",
    "        return True\n",
    "\n",
    "def test_08_moe_router_forward_pass():\n",
    "    \"\"\"Tests MoERouter's forward pass.\"\"\"\n",
    "    # Router expects (1 + raw_event_feature_dim) as input_dim\n",
    "    router_input_dim_calc = 1 + DEFAULT_CONFIG.raw_event_feature_dim if DEFAULT_CONFIG.use_aux_features_router else 1\n",
    "    router = MoERouter(input_dim=router_input_dim_calc, num_experts=4, config=DEFAULT_CONFIG).to(DEVICE)\n",
    "    router.eval()\n",
    "\n",
    "    batch_size = 8\n",
    "    timestamp_input = torch.randn(batch_size, 1, device=DEVICE, dtype=DTYPE)\n",
    "    \n",
    "    # FIX: Prepare router_input according to use_aux_features_router\n",
    "    if DEFAULT_CONFIG.use_aux_features_router and DEFAULT_CONFIG.raw_event_feature_dim > 0:\n",
    "        auxiliary_features_for_router = torch.randn(batch_size, DEFAULT_CONFIG.raw_event_feature_dim, device=DEVICE, dtype=DTYPE)\n",
    "        # Pass the auxiliary features explicitly\n",
    "        logits, weights = router(timestamp_input, auxiliary_features_for_router)\n",
    "    else:\n",
    "        # Pass None if aux features are not used\n",
    "        logits, weights = router(timestamp_input, None) # This path already works fine for input_dim=1\n",
    "\n",
    "    assert logits.shape == (batch_size, 4), \"Router logits shape mismatch.\"\n",
    "    assert weights.shape == (batch_size, 4), \"Router weights shape mismatch.\"\n",
    "    assert torch.allclose(weights.sum(dim=-1), torch.ones(batch_size, device=DEVICE)), \"Router weights do not sum to 1.\"\n",
    "    assert not torch.isnan(logits).any(), \"NaN in router logits.\"\n",
    "    print(\"   MoERouter forward pass successful.\")\n",
    "    return True\n",
    "\n",
    "# ... (tests 09, 10, 11 - no changes to these tests' logic) ...\n",
    "# (The prints I added for KANLayer and K_MOTE still need to be in your\n",
    "#  src/layers/kan_base_layer.py and src/models/k_mote.py files)\n",
    "\n",
    "def test_10_dynamic_mamba_ssm_step_method():\n",
    "    \"\"\"Tests DynamicMambaSSM's step method for single-token processing with states.\"\"\"\n",
    "    import traceback\n",
    "\n",
    "    print(\"\\n=== DynamicMambaSSM Step Method Test ===\")\n",
    "    d_model_mamba = DEFAULT_CONFIG.d_model\n",
    "    k_mote_delta_t_embedding_dim = DEFAULT_CONFIG.D_time\n",
    "\n",
    "    print(\"-> Initializing DynamicMambaSSM module...\")\n",
    "    mamba_ssm = DynamicMambaSSM(\n",
    "        d_model=d_model_mamba,\n",
    "        k_mote_delta_t_embedding_dim=k_mote_delta_t_embedding_dim,\n",
    "        d_state=DEFAULT_CONFIG.mamba_d_state,\n",
    "        d_conv=DEFAULT_CONFIG.mamba_d_conv,\n",
    "        expand=DEFAULT_CONFIG.mamba_expand,\n",
    "        headdim=DEFAULT_CONFIG.mamba_headdim,\n",
    "        dt_min=DEFAULT_CONFIG.mamba_dt_min,\n",
    "        dt_max=DEFAULT_CONFIG.mamba_dt_max,\n",
    "        dt_init_floor=DEFAULT_CONFIG.mamba_dt_init_floor,\n",
    "        bias=DEFAULT_CONFIG.mamba_bias,\n",
    "        conv_bias=DEFAULT_CONFIG.mamba_conv_bias,\n",
    "        d_ssm=DEFAULT_CONFIG.mamba_d_ssm,\n",
    "        device=DEVICE,\n",
    "        dtype=DTYPE\n",
    "    ).to(DEVICE)\n",
    "    mamba_ssm.eval()\n",
    "\n",
    "    batch_size = 8\n",
    "    print(\"-> Generating input tensors...\")\n",
    "    hidden_states_step = torch.randn(batch_size, 1, d_model_mamba, device=DEVICE, dtype=DTYPE)\n",
    "    d_state = DEFAULT_CONFIG.mamba_d_state\n",
    "    d_conv = DEFAULT_CONFIG.mamba_d_conv\n",
    "    mamba_expand = DEFAULT_CONFIG.mamba_expand\n",
    "    d_inner_effective = mamba_expand * d_model_mamba\n",
    "    d_ssm_effective = d_inner_effective if DEFAULT_CONFIG.mamba_d_ssm is None else DEFAULT_CONFIG.mamba_d_ssm\n",
    "    ngroups_effective = 1\n",
    "    conv_channels_for_state = d_ssm_effective + 2 * ngroups_effective * d_state\n",
    "    nheads = d_ssm_effective // DEFAULT_CONFIG.mamba_headdim\n",
    "    dt_modulation_step_input = torch.randn(batch_size, nheads, device=DEVICE, dtype=DTYPE)\n",
    "    conv_state = torch.zeros(batch_size, conv_channels_for_state, d_conv - 1, device=DEVICE, dtype=DTYPE)\n",
    "    initial_conv_state_copy = conv_state.clone().detach()\n",
    "    ssm_state = torch.zeros(batch_size, nheads, DEFAULT_CONFIG.mamba_headdim, d_state, device=DEVICE, dtype=DTYPE)\n",
    "    initial_ssm_state_copy = ssm_state.clone().detach()\n",
    "\n",
    "    print(f\"   [Input] hidden_states_step shape: {hidden_states_step.shape}, min: {hidden_states_step.min().item():.4f}, max: {hidden_states_step.max().item():.4f}\")\n",
    "    print(f\"   [Input] dt_modulation_step_input shape: {dt_modulation_step_input.shape}\")\n",
    "    print(f\"   [State] conv_state shape: {conv_state.shape}, sum: {conv_state.sum().item():.4f}\")\n",
    "    print(f\"   [State] ssm_state shape: {ssm_state.shape}, sum: {ssm_state.sum().item():.4f}\")\n",
    "\n",
    "    try:\n",
    "        print(\"-> Running DynamicMambaSSM.step()...\")\n",
    "        with torch.no_grad():\n",
    "            output, next_conv_state, next_ssm_state = mamba_ssm.step(\n",
    "                hidden_states=hidden_states_step,\n",
    "                conv_state=conv_state,\n",
    "                ssm_state=ssm_state,\n",
    "                dt_modulation_step=dt_modulation_step_input\n",
    "            )\n",
    "\n",
    "        print(f\"   [Output] output shape: {output.shape}, min: {output.min().item():.4f}, max: {output.max().item():.4f}, abs_sum: {output.abs().sum().item():.4f}\")\n",
    "        print(f\"   [State] next_conv_state shape: {next_conv_state.shape}, changed: {not torch.equal(initial_conv_state_copy, next_conv_state)}\")\n",
    "        print(f\"   [State] next_ssm_state shape: {next_ssm_state.shape}, changed: {not torch.equal(initial_ssm_state_copy, next_ssm_state)}\")\n",
    "\n",
    "        assert output.shape == (batch_size, 1, d_model_mamba), f\"DynamicMambaSSM step output shape mismatch: {output.shape}\"\n",
    "        assert next_conv_state.shape == conv_state.shape, f\"DynamicMambaSSM next_conv_state shape mismatch: {next_conv_state.shape}\"\n",
    "        assert next_ssm_state.shape == ssm_state.shape, f\"DynamicMambaSSM next_ssm_state shape mismatch: {next_ssm_state.shape}\"\n",
    "        assert not torch.equal(initial_conv_state_copy, next_conv_state), \"Conv state did not update in DynamicMambaSSM.step.\"\n",
    "        assert not torch.equal(initial_ssm_state_copy, next_ssm_state), \"SSM state did not update in DynamicMambaSSM.step.\"\n",
    "        assert not torch.isnan(output).any(), \"NaN in DynamicMambaSSM step output.\"\n",
    "\n",
    "        print(\"✅ DynamicMambaSSM step method successful with state updates and valid output.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"❌ DynamicMambaSSM step method FAILED.\")\n",
    "        print(f\"   Error: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def test_11_regularization_losses():\n",
    "    \"\"\"Tests the regularization loss computations.\"\"\"\n",
    "    reg_handler = KANMAMMOTE_RegularizationLosses(DEFAULT_CONFIG)\n",
    "    \n",
    "    # Test load_balance_loss\n",
    "    batch_size = 16\n",
    "    num_experts = 4 # Hardcoded in K_MOTE\n",
    "    dummy_expert_weights = torch.rand(batch_size, num_experts, device=DEVICE, dtype=DTYPE)\n",
    "    # Make them sum to 1 per row for realistic softmax weights\n",
    "    dummy_expert_weights = dummy_expert_weights / dummy_expert_weights.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    load_loss = reg_handler.compute_load_balance_loss(dummy_expert_weights)\n",
    "    assert isinstance(load_loss, torch.Tensor), \"Load balance loss is not a tensor.\"\n",
    "    assert load_loss.item() >= 0, \"Load balance loss should be non-negative.\"\n",
    "    print(f\"   Load Balance Loss computed: {load_loss.item():.4f}\")\n",
    "\n",
    "    # Test Sobolev and Total Variation (just check if they run without error as they are stubs)\n",
    "    model_for_stub_loss = KANMAMMOTE(DEFAULT_CONFIG).to(DEVICE) # Need a model instance for these stubs\n",
    "    sobolev_loss = reg_handler.compute_sobolev_l2_loss(model_for_stub_loss)\n",
    "    total_variation_loss = reg_handler.compute_total_variation_loss(model_for_stub_loss)\n",
    "    \n",
    "    assert isinstance(sobolev_loss, torch.Tensor) and sobolev_loss.item() == 0.0, \"Sobolev loss stub not working as expected.\"\n",
    "    assert isinstance(total_variation_loss, torch.Tensor) and total_variation_loss.item() == 0.0, \"Total Variation loss stub not working as expected.\"\n",
    "    print(\"   Sobolev L2 and Total Variation loss stubs ran successfully.\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    all_tests_passed = True\n",
    "    print(\"--- Starting KAN-MAMMOTE Component Tests ---\")\n",
    "    print(f\"Running tests on device: {DEVICE} with dtype: {DTYPE}\")\n",
    "    print(f\"Using default config: d_model={DEFAULT_CONFIG.d_model}, D_time={DEFAULT_CONFIG.D_time}, num_layers={DEFAULT_CONFIG.num_layers}\")\n",
    "    #print config device\n",
    "    print(f\"KANMAMMOTEConfig: {DEFAULT_CONFIG}\")\n",
    "    print(f\"KANMAMMOTEConfig device: {DEFAULT_CONFIG.device}, dtype: {DEFAULT_CONFIG.dtype}\")\n",
    "    # List of tests to run\n",
    "    tests = [\n",
    "        test_01_config_initialization,\n",
    "        test_02_kan_mammote_initialization,\n",
    "        test_03_kan_mammote_forward_pass_full_sequence,\n",
    "        test_04_kan_mammote_backward_pass_full_sequence,\n",
    "        test_05_kan_mammote_training_step,\n",
    "        test_06_continuous_mamba_block_forward, # Tests the updated forward with states\n",
    "        test_07_k_mote_forward_pass,\n",
    "        test_08_moe_router_forward_pass,\n",
    "        test_09_basis_functions_forward_pass,\n",
    "        test_10_dynamic_mamba_ssm_step_method, # Critical for recurrent flow\n",
    "    ]\n",
    "\n",
    "    for test_func in tests:\n",
    "        if not run_test(test_func.__name__.replace('test_', '').replace('_', ' ').title(), test_func):\n",
    "            all_tests_passed = False\n",
    "\n",
    "    print(\"\\n--- All Component Tests Finished ---\")\n",
    "    if all_tests_passed:\n",
    "        print(\"✅ CONGRATULATIONS! All KAN-MAMMOTE components appear to be working correctly.\")\n",
    "    else:\n",
    "        print(\"❌ WARNING: Some KAN-MAMMOTE components failed or completed with warnings. Please review the logs above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd4acc8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ea9af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kanmote_wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
